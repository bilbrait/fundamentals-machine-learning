{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3537863a",
   "metadata": {},
   "source": [
    "#### 1- See how round-off errors behave when computing a derivative of a function. Use $f_0 = sin( x_0 )$ and compute its derivative with different discretization ranges. Experiment with different points $x_0$ of differentiation.\n",
    "\n",
    "Steps:\n",
    "+ Choose a point of differentation $x_0$\n",
    "+ Choose a function $f_0$. Here $sin$ is given.\n",
    "+ Assign the derivative $f_0$ to, for example, $f_p$\n",
    "+ Choose a range $i = [ -20, 0 ]$, with 0.5 steps, of discretizations $h = 10^{(\\text{list of i})}$\n",
    "+ Compute the absolute error: $|f_p - \\frac{f_0( x_0+h ) - f_0 }{h}|$\n",
    "+ Compare the absolute error against the discretization error without round-off errors $\\frac{1}{2}|f^{''}(x_0)|h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.2\n",
    "f0 = \n",
    "fp = \n",
    "i  = np.arange()\n",
    "h  = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "err =  # Tip: Use list comprehension\n",
    "derr = f0 / 2 * h # Discretization error without roundoff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog( h, ?, '-ob' )\n",
    "plt.loglog( h, ?, '--r' )\n",
    "plt.title( f'Discretation and round-off error: x0: {x0}, f0: Sin(x0), fp: Cos(x0)')\n",
    "plt.xlabel( 'Discretization h' )\n",
    "plt.ylabel( 'Absolute error' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da16fc5",
   "metadata": {},
   "source": [
    "#### 2- Linear least-squares revisited on polynomial fitting with normal equations. Compare the polynomial coefficients to the backpropagation method from the previous exercise.\n",
    "\n",
    "Solve the algebraic problem of $\\underset{\\vec{x}}{\\text{min}} ||\\vec{b}-A\\vec{x}||_2, \\ A \\in \\mathbb{R}^{m \\times n}, \\ \\vec{x} \\in \\mathbb{R}^n, \\ \\vec{b} \\in \\mathbb{R}^m, m \\geq n$\n",
    "\n",
    "+ Create $B = A^T A$ (a), and $\\vec{y} = A^T \\vec{b}$ (b)\n",
    "+ Use Cholesky Factorization for solving $B$. That is, for $B = GG^T$:\n",
    "    + Solve lower triangular system $G\\vec{z} = \\vec{y}$ for $\\vec{z}$ (c)\n",
    "    + Solve upper triangular system $G^T\\vec{x} = \\vec{z}$ for $\\vec{x}$ (d)\n",
    "    + Reminder/tip: Lower triangular system: below the diagnoal of a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(t, b, n):\n",
    "    t = t.reshape(-1, 1)\n",
    "    b = b.reshape(-1, 1)\n",
    "    m = # Hint: number of equidistant points\n",
    "    A = # Hint: Contains all ones\n",
    "    for j in range(1, n):\n",
    "        A[:, j] = A[:, j-1] * t.flatten()\n",
    "    B = ? @  # (a)\n",
    "    y = ? @  # (b)\n",
    "    coefs = np.linalg.solve(B, y)\n",
    "    return coefs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ?\n",
    "d = ? # max degree of polynomial fitting\n",
    "tt = np.linspace(0, 1, m)\n",
    "bb = np.cos(2 * np.pi * tt) # Try out other functions like sine\n",
    "\n",
    "coefs = {}\n",
    "for n in range(1, ?):\n",
    "    coefs[n] = least_squares_fit(tt, bb, n)\n",
    "\n",
    "t = np.linspace( 0, 1, 101 )\n",
    "z = np.ones( ( ?, 101 ) )\n",
    "# (c) lower triangular G (Cholesky factor)\n",
    "for n in range( 1, ? ):\n",
    "    z[n-1, :] = z[n-1, :] * ?\n",
    "    # (d) upper triangular G^T (Cholesky factor)\n",
    "    for j in range( ?, ?, ?):\n",
    "        z[n-1, :] = z[n-1, :] * t + ?\n",
    "\n",
    "plt.plot(t, ?, tt, bb, 'ro')\n",
    "plt.plot( tt, bb, label = 'data')\n",
    "plt.legend()\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('p_{n-1}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print polynomial coefficients from using backpropagation and least-squares with normal equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b232955d",
   "metadata": {},
   "source": [
    "#### 3- Eigenvalue solvers for blind source separation: How sensitive Principal Component Analysis (PCA) and Independent Component Analysis (ICA) are to the number of observations, given a random process? What changes the most in both PCA and ICA projections?\n",
    "\n",
    "##### Solving the eigenvalue problem $A\\vec{x} = \\lambda \\vec{x}, \\ A \\in \\mathbb{R}^2, \\ \\vec{x}, \\lambda \\in \\mathbb{R}$\n",
    "\n",
    "##### Both PCA and ICA:uses Singular Value Decomposition method for extracting relevant eigenvalues and vectors from a given data: $A = U \\Sigma V^T$, where $U,V$ are the left and right orthonormal bases vectors of the eigenvalues $\\Sigma$, which is sorted from high to low. \n",
    "\n",
    "Try out:\n",
    "+ Student t Distribution ( https://en.wikipedia.org/wiki/Student's_t-distribution )\n",
    "+ Pareto Distribution ( https://en.wikipedia.org/wiki/Pareto_distribution )\n",
    "+ As a bonus 1: check other distributions from np_rng class. Use external sources to checkout properties of other distributions if you're going to use other than the two above.\n",
    "+ Bonus 2: Check out the ratio of the largest and smallest eigenvalues of the observed data, using np.linalg.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781745a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "np_rng = np.random.RandomState( 4432 )\n",
    "\n",
    "S      = np_rng.standard_t(1.5, size=(20000, 2)) # Student t Distribution\n",
    "S[:, 0] *= 2.\n",
    "\n",
    "# Mix data\n",
    "A = np.array([[1, 0], [0, 2]])  # Mixing matrix\n",
    "\n",
    "# Generate observations by computing the matrix product SA^T\n",
    "X = np.dot(S, A.T)  \n",
    "\n",
    "pca    = PCA()\n",
    "S_pca_ = pca.fit(X).transform(X)\n",
    "\n",
    "ica    = FastICA(random_state=rng.integers( 42 ) )\n",
    "S_ica_ = ica.fit(X).transform(X)  # Estimate the sources\n",
    "\n",
    "# Scale row-wise using standard deviation of S_ica_\n",
    "S_ica_ /= S_ica_.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e42b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(S, axis_list=None):\n",
    "    plt.scatter(S[:, 0], S[:, 1], s=2, marker='o', zorder=10,\n",
    "                color='steelblue', alpha=0.5)\n",
    "    if axis_list is not None:\n",
    "        colors = ['orange', 'red']\n",
    "        for color, axis in zip(colors, axis_list):\n",
    "            axis /= axis.std()\n",
    "            x_axis, y_axis = axis\n",
    "            # Trick to get legend to work\n",
    "            plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color)\n",
    "            plt.quiver((0, 0), (0, 0), x_axis, y_axis, zorder=11, width=0.01,\n",
    "                       scale=6, color=color)\n",
    "\n",
    "    plt.hlines(0, -3, 3)\n",
    "    plt.vlines(0, -3, 3)\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85125c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 14, 9 ) )\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_samples(S / S.std())\n",
    "plt.title('True Independent Sources')\n",
    "\n",
    "# PCA components and ICA mixing components\n",
    "axis_list = [pca.components_.T, ica.mixing_]\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_samples(X / np.std(X), axis_list=axis_list)\n",
    "legend = plt.legend(['PCA', 'ICA'], loc='upper right')\n",
    "legend.set_zorder(100)\n",
    "\n",
    "plt.title('Observations')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_samples(S_pca_ / np.std(S_pca_, axis=0))\n",
    "plt.title('PCA recovered signals')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_samples(S_ica_ / np.std(S_ica_))\n",
    "plt.title('ICA recovered signals')\n",
    "\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f65808-58fd-46db-bd8b-9461f171f8b3",
   "metadata": {},
   "source": [
    "#### 4- Implement a linear regression using automatic differentiation. Try with different learning rates and number of iterations for the gradient descent steps. How sensitive the model is to the learning rate and iterations?\n",
    "+ (a) implement Wx + b\n",
    "+ (b) implement Mean squared error ( hint: np.square )\n",
    "+ (c) vary number of points n and noise level. How sensitive the method is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cc595-a491-4866-90e7-0eedc4875d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77c23b-c021-43ec-8a4b-050ccc1bf3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037688c2-5093-4789-a7f5-69e22e78c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear regression model: Xw + b ( hint: dot product )\n",
    "def model(X, w, b):\n",
    "    return ? # (a)\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "def loss(params, X, y):\n",
    "    return np.mean( ? ) # (b)\n",
    "\n",
    "# Create a gradient function for the loss function\n",
    "loss_grad = grad( ? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf80ec9-92bf-4855-b6c4-d39839f5aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data using a list of integers\n",
    "n     = 10 # (c)\n",
    "X     = np.arange(n).reshape(-1, 1)  # Reshape to 10x1 vector\n",
    "noise = np.random.randn(n, 1) # Add some noise (c)\n",
    "y      = 2 * X + 1 + noise\n",
    "\n",
    "print('Num of data points:', n)\n",
    "print('X shape:', X.shape)\n",
    "print('noise shape:', noise.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957c7f7-9597-489b-9c62-7d1ab8002d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases randomly\n",
    "w = np.random.rand(1, 1)\n",
    "b = np.random.rand(1, 1)\n",
    "\n",
    "# Perform gradient descent\n",
    "learning_rate = 0.01\n",
    "for i in range(100):\n",
    "    grad_w, grad_b = loss_grad([w, b], X, y)\n",
    "    print(f'i: {i}, w={w}, loss={loss([w, b], X, y)}, grad_w={grad_w}, grad_b={grad_b}')\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "# Print the learned weights and biases\n",
    "print(\"Learned w:\", w)\n",
    "print(\"Learned b:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca90da-e975-41e1-8064-5c27d8436526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the same data\n",
    "predictions = model(X, w, b)\n",
    "\n",
    "# Compare predicted and real values\n",
    "print(\"Predicted values:\", predictions)\n",
    "print(\"Actual values:\", y)\n",
    "\n",
    "# Visualize the comparison (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, y, 'o', label='Actual data')\n",
    "plt.plot(X, predictions, '-x', label='Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
