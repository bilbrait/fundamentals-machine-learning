{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ac7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.datasets as data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' ) # use whitegrid if prefer a white background\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ec631",
   "metadata": {},
   "source": [
    "#### 1- Use the Diabetes data set to perform a regression task using Decision trees, Gradient boosting, Multi-layered Perceptron and SVM. Compare the two models by inspecting the decision path done by the Decision tree, feature importanceby Gradient boosting, support vectors of the SVM and partial featuer dependence of MLPs. Which gives more intepretable results in your mind? Vary different parameters in both models and compare the resulting modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = data.load_diabetes()\n",
    "X, y = data.load_diabetes( return_X_y = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe4017",
   "metadata": {},
   "source": [
    "#### Diabetes data\n",
    "\n",
    "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "Data Set Characteristics:\n",
    "\n",
    "Number of Instances\n",
    "442\n",
    "\n",
    "Number of Attributes\n",
    "First 10 columns are numeric predictive values\n",
    "\n",
    "Target\n",
    "Column 11 is a quantitative measure of disease progression one year after baseline\n",
    "\n",
    "Attribute Information\n",
    "age age in years\n",
    "\n",
    "sex\n",
    "\n",
    "bmi body mass index\n",
    "\n",
    "bp average blood pressure\n",
    "\n",
    "s1 tc, total serum cholesterol\n",
    "\n",
    "s2 ldl, low-density lipoproteins\n",
    "\n",
    "s3 hdl, high-density lipoproteins\n",
    "\n",
    "s4 tch, total cholesterol / HDL\n",
    "\n",
    "s5 ltg, possibly log of serum triglycerides level\n",
    "\n",
    "s6 glu, blood sugar level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec590e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 666 )\n",
    "\n",
    "# Define model, do a model fit, and print out train and test scores\n",
    "\n",
    "\n",
    "plt.figure( figsize = ( 20, 10 ) )\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15419363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same above but for the whole dataset, not train/test split\n",
    "\n",
    "plt.figure( figsize = ( 20, 10 ) )\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ead8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import \n",
    "f\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.1, random_state=13 )\n",
    "\n",
    "gb = GradientBoostingRegressor()\n",
    "gb.fit( X, y ) # Try also with the train/test split\n",
    "\n",
    "# print the mean squared error of model fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e53e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = gb.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, np.array(diabetes.feature_names)[sorted_idx])\n",
    "plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "# Try different n_repeats\n",
    "result = permutation_importance(\n",
    "    gb, X, y, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(diabetes.feature_names)[sorted_idx],\n",
    ")\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591bd17",
   "metadata": {},
   "source": [
    "#### 2- Compare applying regularization on the Diabates dataset and Breast Cancer data using SVM, ANNs and Gradient Boosting\n",
    "+ Plot similar training and test error curves as in FML 2- SVMs and ANNs\n",
    "+ Plot feature importance from Gradient Boosting, and extract the support vectors from SVM and compute the extracted data's statistics (as in FML 2 notebook). For MLP, use partial dependece plot (code given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data.load_breast_cancer( return_X_y = True, as_frame = True )\n",
    "print( X.shape )\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup here train and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split( X, Y, test_size = 0.3, random_state = rng.integers( 342 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# To keep it simple, vary the number of estimators and min number of samples before splitting\n",
    "# Start with varying number of estimators while keeping min_sample_spit constant. Then switch around\n",
    "param_range = np.arange( 10, 20 )\n",
    "train_e     = [] # errors\n",
    "test_e      = []\n",
    "\n",
    "for a in param_range:\n",
    "    ?.set_params( n_estimators = a )\n",
    "    ?.fit( Xtrain, ytrain )\n",
    "    \n",
    "    \n",
    "i_optim = np.argmax( ? )\n",
    "optim   = param_range[i_optim]\n",
    "print(\"Optimal regularization parameter : %s\" % optim)\n",
    "\n",
    "?.set_params( n_estimators = optim )\n",
    "?.fit( X, Y )\n",
    "\n",
    "\n",
    "plt.figure( figsize = ( 18, 10 ) )\n",
    "plt.title( 'Gradient Boosting ')\n",
    "plt.semilogx(param_range, train_e, label='Train')\n",
    "plt.semilogx(param_range, test_e, label='Test')\n",
    "plt.vlines(optim, plt.ylim()[0], np.max(test_e), color='k',\n",
    "           linewidth=5, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([min(test_e)-0.1, max(train_e)+0.1])\n",
    "plt.xlabel('Parameter range')\n",
    "plt.ylabel('Performance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54634ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = gb.feature_importances_\n",
    "sorted_idx         = np.argsort(feature_importance)\n",
    "pos                = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, np.array(X.columns)[sorted_idx])\n",
    "plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "result = permutation_importance(\n",
    "    gb, Xtrain, ytrain, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(X.columns)[sorted_idx],\n",
    ")\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb19f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier( hidden_layer_sizes = 10, activation = 'logistic', solver = 'sgd' )\n",
    "\n",
    "param_range = np.logspace( )\n",
    "\n",
    "\n",
    "for a in param_range:\n",
    "    ?.set_params( alpha = a )\n",
    "    ?.fit( Xtrain, ytrain )\n",
    "    \n",
    "    \n",
    "i_optim = np.argmax(?)\n",
    "optim   = param_range[i_optim]\n",
    "print(\"Optimal regularization parameter : %s\" % optim)\n",
    "\n",
    "mlp.set_params( alpha = optim )\n",
    "mlp.fit( X, Y )\n",
    "\n",
    "plt.figure( figsize = ( 18, 10 ) )\n",
    "plt.title( 'Multi-layered Perceptron')\n",
    "plt.semilogx(param_range, train_e, label='Train')\n",
    "plt.semilogx(param_range, test_e, label='Test')\n",
    "plt.vlines(optim, plt.ylim()[0], np.max(test_e), color='k',\n",
    "           linewidth=5, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([min(test_e)-0.1, max(train_e)+0.1])\n",
    "plt.xlabel('Parameter range')\n",
    "plt.ylabel('Performance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cbf1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "\n",
    "# Pick the top four features from the results from Gradient Boosting feature importance\n",
    "features = [ 'worst radius', 'worst perimeter', 'mean concave points', 'worst concave points' ]\n",
    "\n",
    "display = plot_partial_dependence(\n",
    "       mlp, Xtrain, features, kind=\"both\", subsample=20,\n",
    "       n_jobs=3, grid_resolution=20, random_state=0\n",
    ")\n",
    "\n",
    "display.figure_.suptitle(\n",
    "    'Comparing Partial dependence of top features found by Gradient Boosting'\n",
    ")\n",
    "display.figure_.subplots_adjust(hspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Same as FML 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
