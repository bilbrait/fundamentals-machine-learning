{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ac7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.datasets as data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' ) # use whitegrid if prefer a white background\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#matplotlib.rcParams.update( { 'font.size': 18 } ) # Use this to setup your preferred font size for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ec631",
   "metadata": {},
   "source": [
    "#### 1- Use the Diabetes data set to perform a regression task using Decision trees, Gradient boosting, Multi-layered Perceptron and SVM. Compare the two models by inspecting the decision path done by the Decision tree, feature importanceby Gradient boosting, support vectors of the SVM and partial featuer dependence of MLPs. Which gives more intepretable results in your mind? Vary different parameters in both models and compare the resulting modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = data.load_diabetes()\n",
    "X, Y = data.load_diabetes( return_X_y = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe4017",
   "metadata": {},
   "source": [
    "#### Diabetes data\n",
    "\n",
    "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "Data Set Characteristics:\n",
    "\n",
    "Number of Instances\n",
    "442\n",
    "\n",
    "Number of Attributes\n",
    "First 10 columns are numeric predictive values\n",
    "\n",
    "Target\n",
    "Column 11 is a quantitative measure of disease progression one year after baseline\n",
    "\n",
    "Attribute Information\n",
    "age age in years\n",
    "\n",
    "sex\n",
    "\n",
    "bmi body mass index\n",
    "\n",
    "bp average blood pressure\n",
    "\n",
    "s1 tc, total serum cholesterol\n",
    "\n",
    "s2 ldl, low-density lipoproteins\n",
    "\n",
    "s3 hdl, high-density lipoproteins\n",
    "\n",
    "s4 tch, total cholesterol / HDL\n",
    "\n",
    "s5 ltg, possibly log of serum triglycerides level\n",
    "\n",
    "s6 glu, blood sugar level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec590e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state = 666 )\n",
    "\n",
    "clf = DecisionTreeRegressor( max_leaf_nodes=5, random_state = 34)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print( \"Training score: {0}\".format( clf.score( X_train, y_train ) ) )\n",
    "print( \"Test score: {0}\".format( clf.score(X_test, y_test ) ) )\n",
    "\n",
    "plt.figure( figsize = ( 20, 10 ) )\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15419363",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, Y)\n",
    "\n",
    "print( \"Training score: {0}\".format( clf.score( X, Y ) ) )\n",
    "\n",
    "plt.figure( figsize = ( 20, 10 ) )\n",
    "tree.plot_tree(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ead8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.1, random_state=13 )\n",
    "\n",
    "gb = GradientBoostingRegressor()\n",
    "gb.fit( X, Y ) # Try also with the train/test split\n",
    "\n",
    "# print the mean squared error of model fitting\n",
    "print( \"The mean squared error (MSE) on test set: {:.4f}\".format( mean_squared_error( Y, gb.predict( X ) ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e53e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = gb.feature_importances_\n",
    "sorted_idx         = np.argsort(feature_importance)\n",
    "pos                = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, np.array(diabetes.feature_names)[sorted_idx])\n",
    "plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "result = permutation_importance(\n",
    "    gb, X, Y, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(diabetes.feature_names)[sorted_idx],\n",
    ")\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591bd17",
   "metadata": {},
   "source": [
    "#### 2- Compare applying regularization on the Diabates dataset and Breast Cancer data using SVM, ANNs and Gradient Boosting\n",
    "+ Plot similar training and test error curves as in FML 2- SVMs and ANNs\n",
    "+ Plot feature importance from Gradient Boosting, and extract the support vectors from SVM and compute the extracted data's statistics (as in FML 2 notebook). For MLP, use partial dependece plot (code given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data.load_breast_cancer( return_X_y = True, as_frame = True )\n",
    "print( X.shape )\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup here train and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split( X, Y, test_size = 0.3, random_state = rng.integers( 342 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb  = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "# To keep it simple, vary the number of estimators and min number of samples before splitting\n",
    "# Start with varying number of estimators while keeping min_sample_spit constant. Then switch around\n",
    "param_range = np.arange( 10, 20 )\n",
    "train_e     = []\n",
    "test_e      = []\n",
    "\n",
    "for a in param_range:\n",
    "    gb.set_params( n_estimators = a )\n",
    "    gb.fit( Xtrain, ytrain )\n",
    "    train_e.append( gb.score( Xtrain, ytrain ) )\n",
    "    test_e.append( gb.score( Xtest, ytest ) )\n",
    "    \n",
    "i_optim = np.argmax(test_e)\n",
    "optim   = param_range[i_optim]\n",
    "print(\"Optimal regularization parameter : %s\" % optim)\n",
    "\n",
    "gb.set_params( n_estimators = optim )\n",
    "gb.fit( X, Y )\n",
    "\n",
    "\n",
    "plt.figure( figsize = ( 18, 10 ) )\n",
    "plt.title( 'Gradient Boosting ')\n",
    "plt.semilogx(param_range, train_e, label='Train')\n",
    "plt.semilogx(param_range, test_e, label='Test')\n",
    "plt.vlines(optim, plt.ylim()[0], np.max(test_e), color='k',\n",
    "           linewidth=5, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([min(test_e)-0.1, max(train_e)+0.1])\n",
    "plt.xlabel('Parameter range')\n",
    "plt.ylabel('Performance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a83263",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = gb.feature_importances_\n",
    "sorted_idx         = np.argsort(feature_importance)\n",
    "pos                = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, np.array(X.columns)[sorted_idx])\n",
    "plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "result = permutation_importance(\n",
    "    gb, Xtrain, ytrain, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(X.columns)[sorted_idx],\n",
    ")\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82522df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier( hidden_layer_sizes = 10, activation = 'logistic', solver = 'sgd' )\n",
    "\n",
    "param_range = np.logspace( -2, 1, 3 )\n",
    "train_e     = []\n",
    "test_e      = []\n",
    "\n",
    "for a in param_range:\n",
    "    mlp.set_params( alpha = a )\n",
    "    mlp.fit( Xtrain, ytrain )\n",
    "    train_e.append( gb.score( Xtrain, ytrain ) )\n",
    "    test_e.append( gb.score( Xtest, ytest ) )\n",
    "    \n",
    "i_optim = np.argmax(test_e)\n",
    "optim   = param_range[i_optim]\n",
    "print(\"Optimal regularization parameter : %s\" % optim)\n",
    "\n",
    "mlp.set_params( alpha = optim )\n",
    "mlp.fit( X, Y )\n",
    "\n",
    "plt.figure( figsize = ( 18, 10 ) )\n",
    "plt.title( 'Multi-layered Perceptron')\n",
    "plt.semilogx(param_range, train_e, label='Train')\n",
    "plt.semilogx(param_range, test_e, label='Test')\n",
    "plt.vlines(optim, plt.ylim()[0], np.max(test_e), color='k',\n",
    "           linewidth=5, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([min(test_e)-0.1, max(train_e)+0.1])\n",
    "plt.xlabel('Parameter range')\n",
    "plt.ylabel('Performance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a34c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "\n",
    "# Pick the top four features from the results from Gradient Boosting feature importance\n",
    "features = [ 'worst radius', 'worst perimeter', 'mean concave points', 'worst concave points' ]\n",
    "\n",
    "display = plot_partial_dependence(\n",
    "       mlp, Xtrain, features, kind=\"both\", subsample=20,\n",
    "       n_jobs=3, grid_resolution=20, random_state=0\n",
    ")\n",
    "\n",
    "display.figure_.suptitle(\n",
    "    'Comparing Partial dependence of top features found by Gradient Boosting'\n",
    ")\n",
    "display.figure_.subplots_adjust(hspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "param_range = np.logspace( -10, 1, 10 ) # gamma parameter\n",
    "C           = 122.0\n",
    "svc         = SVC( C = C, kernel = 'linear' )\n",
    "train_e     = []\n",
    "test_e      = []\n",
    "\n",
    "for a in param_range:\n",
    "    svc.set_params( gamma = a )\n",
    "    svc.fit( Xtrain, ytrain )\n",
    "    train_e.append( svc.score( Xtrain, ytrain ) )\n",
    "    test_e.append( svc.score( Xtest, ytest ) )\n",
    "    \n",
    "i_optim = np.argmax(test_e)\n",
    "optim   = param_range[i_optim]\n",
    "print(\"Optimal regularization parameter : %s\" % optim)\n",
    "\n",
    "svc.set_params( gamma = optim)\n",
    "svc.fit( Xtrain, ytrain )\n",
    "print( \"Number of support vectors per class:\")\n",
    "print( \"Class 0: {0} | Class 1: {1}\".format( svc.n_support_[ 0 ], svc.n_support_[ 1 ] ) )\n",
    "\n",
    "plt.figure( figsize = ( 18, 10 ) )\n",
    "plt.title( 'Linear SVM')\n",
    "plt.semilogx(param_range, train_e, label='Train')\n",
    "plt.semilogx(param_range, test_e, label='Test')\n",
    "plt.vlines(optim, plt.ylim()[0], np.max(test_e), color='k',\n",
    "           linewidth=5, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([min(test_e)-0.1, max(train_e)+0.1])\n",
    "plt.xlabel('Parameter range')\n",
    "plt.ylabel('Performance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44925858",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsv = Xtrain.iloc[ svc.support_ ]\n",
    "Xsv[ features ].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
