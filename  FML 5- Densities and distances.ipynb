{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36dacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.datasets as data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' ) # use whitegrid if prefer a white background\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#matplotlib.rcParams.update( { 'font.size': 18 } ) # Use this to setup your preferred font size for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18a92a",
   "metadata": {},
   "source": [
    "#### 1- First estimate different covariances on the Iris dataset using Gaussian Mixture Models. Plot the different covriance estimation and how well they fit to the data with different number of components and iterations.\n",
    "\n",
    "+ Bonus: use np.random.norma( mu, sigma, X.shape ) and see how noise affects the estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaafefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from fml5_helper_funcs import make_ellipses\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0461aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data.load_iris( return_X_y = True )\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split( X, Y, test_size = ?, random_state = rng.integers( 4231 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different number of components in the Gaussian Mixture\n",
    "# model and different amount of iterations\n",
    "\n",
    "GMMs = {cov_type: GaussianMixture( n_components    = ?       ,\n",
    "                                   covariance_type = cov_type, \n",
    "                                   max_iter        = ?       ,\n",
    "                                   random_state = rng.integers( 212 )\n",
    "                                 )\n",
    "              for cov_type in ['spherical', 'diag', 'tied', 'full']\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2770b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes    = ?\n",
    "n_estimators = ?\n",
    "ynames = [ 'Setosa', 'Versicolour', 'Virginica']\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "plt.figure(figsize=(3 * n_estimators // 2, 6))\n",
    "plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n",
    "                    left=.01, right=.99)\n",
    "\n",
    "\n",
    "for index, ( name, estimator ) in enumerate(GMMs.items()):\n",
    "    # Since we have class labels for the training data, we can\n",
    "    # initialize the GMM parameters in a supervised manner.\n",
    "    estimator.means_init = np.array([ ? [ ? == i].mean(axis=0)\n",
    "                                    for i in range(n_classes)])\n",
    "\n",
    "    # Train the other parameters using the EM algorithm.\n",
    "    ?\n",
    "    h = plt.subplot(2, n_estimators // 2, index + 1)\n",
    "    make_ellipses(estimator, h, colors )\n",
    "\n",
    "    for n, color in enumerate(colors):\n",
    "        data = X[ Y == n ]\n",
    "        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n",
    "                    label=ynames[n])\n",
    "    # Plot the test data with crosses\n",
    "    for n, color in enumerate(colors):\n",
    "        data = Xtest[ytest == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n",
    "\n",
    "    ?\n",
    "    train_accuracy = np.mean(y_train_pred.ravel() == ytrain.ravel()) * 100\n",
    "    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,\n",
    "             transform=h.transAxes)\n",
    "\n",
    "    ?\n",
    "    test_accuracy = np.mean(y_test_pred.ravel() == ytest.ravel()) * 100\n",
    "    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,\n",
    "             transform=h.transAxes)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(name)\n",
    "\n",
    "plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24acddb6",
   "metadata": {},
   "source": [
    "#### 2- Perform K-means clustering on the Iris dataset. Try adding Gaussian noise, different $\\mu$ and $\\sigma$, to the observations and see how the noise and covariance structure of the data affects the computing of the clusters. How does this compare to the Gaussian Mixture Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdba02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "range_n_clusters = ?\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    \n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # and get the cluster labels by doing a model fit.\n",
    "    \n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score( ?, ? )\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(?, ?)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range( ? ):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[? == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / ?)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed. See how the clusters look from different feature columns\n",
    "    colors = cm.nipy_spectral(?.astype(float) / ?)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = ?.?\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b277842",
   "metadata": {},
   "source": [
    "#### 3- Use Hierachical clustering to segment coins from a image. Use different numbers of clusters and see which affinity and linkage combination gives the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.transform import rescale\n",
    "from skimage.data import coins\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "orig_coins = coins()\n",
    "smoothened_coins = gaussian_filter(orig_coins, sigma = ? ) # Try also different smoothing levels on the image\n",
    "rescaled_coins = rescale( smoothened_coins, 0.2,mode=\"reflect\",anti_aliasing=False,\n",
    ")\n",
    "\n",
    "Xd = np.reshape(rescaled_coins, (-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "connectivity = grid_to_graph(*rescaled_coins.shape)\n",
    "\n",
    "\n",
    "n_clusters = ?  # number of regions\n",
    "? = AgglomerativeClustering(\n",
    "    n_clusters = ?, affinity = ?, linkage= ?, connectivity=connectivity\n",
    ")\n",
    "?.fit(Xd)\n",
    "label = np.reshape( ?.labels_, rescaled_coins.shape)\n",
    "\n",
    "print(f\"Number of pixels: {label.size}\")\n",
    "print(f\"Number of clusters: {np.unique(label).size}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(rescaled_coins, cmap=plt.cm.gray)\n",
    "for l in range(n_clusters):\n",
    "    plt.contour(\n",
    "        label == l,\n",
    "        colors=[\n",
    "            plt.cm.nipy_spectral(l / float(n_clusters)),\n",
    "        ],\n",
    "    )\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be193802",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( orig_coins )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
