{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ab20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' )\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f71695",
   "metadata": {},
   "source": [
    "#### First some warm up tasks. When in doubt, the help() function is your friend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50883e67",
   "metadata": {},
   "source": [
    "#### 1- Create Gaussian, Exponential and Cauchy random variable ( r.v ) vectors of length $n$. For each vector, plot $\\frac{\\sum_i^k x_i}{k}, k=1..n$. When does the sequence stabilizes and what can you observe?\n",
    "\n",
    "#### ( Do not go $n \\geq 1000000$ if you want to save time. )\n",
    "\n",
    "#### Bonus 1: experiment with other distributions and their parameters. Below are the cumulative distributional functions of the r.vs used in this exercise.\n",
    "\n",
    "#### Bonus 2: experiment by plotting a combination of r.vs. For example, Gaussian + Exponential.\n",
    "\n",
    "+ Gaussian: $\\mathcal{N}(\\mu, \\sigma)$ ($\\mu$: mean, $\\sigma$: standard deviation or scale)\n",
    "+ Caucy: $F(x_0;\\lambda): \\frac{1}{\\pi} \\text{arctan}(\\frac{x-x_0}{\\lambda}) + \\frac{1}{2}$\n",
    "+ Exponential: $F(x;\\lambda) = 1 - {\\rm e}^{-\\lambda x}$ ($\\lambda$: rate parameter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import cauchy\n",
    "from scipy.stats import expon\n",
    "\n",
    "n = 5000\n",
    "\n",
    "Grv1 = norm.rvs( loc = 1, scale = 10.5, size = n, random_state = rng )\n",
    "Grv2 = norm.rvs( size = n, random_state = rng )\n",
    "Crv = cauchy.rvs( loc = -1, scale = 2.5, size = n, random_state = rng )\n",
    "Erv = expon.rvs( size = n, random_state = rng )\n",
    "\n",
    "a1 = []\n",
    "for i in range(len(Grv1)): \n",
    "    if i == 0: a1.append( sum( Grv1[ :i ] )/ 1 )\n",
    "    else: a1.append( sum( Grv1[ :i ] )/ i )\n",
    "\n",
    "a2 = []\n",
    "for i in range(len(Grv2)): \n",
    "    if i == 0: a2.append( sum( Grv2[ :i ] )/ 1 )\n",
    "    else: a2.append( sum( Grv2[ :i ] )/ i )\n",
    "        \n",
    "b = []\n",
    "for i in range(len(Crv)): \n",
    "    if i == 0: b.append( sum( Crv[ :i ] )/ 1 )\n",
    "    else: b.append( sum( Crv[ :i ] )/ i )\n",
    "        \n",
    "\n",
    "c = []\n",
    "for i in range(len(Erv)): \n",
    "    if i == 0: c.append( sum( Erv[ :i ] )/ 1 )\n",
    "    else: c.append( sum( Erv[ :i ] )/ i )\n",
    "        \n",
    "d = []\n",
    "for i in range(len(Erv)):\n",
    "    X = Erv + Crv + Grv1\n",
    "    if i == 0: d.append( sum( X[ :i ] )/ 1 )\n",
    "    else: d.append( sum(  X[ :i ] )/ i )\n",
    "\n",
    "plt.figure( figsize=( 10, 7 ) )\n",
    "plt.plot( a1, label = \"Gaussian, mean = 1, scale = 10.5\" )\n",
    "plt.plot( a2, label = \"Gaussian, mean = 0, scale = 1\" )\n",
    "plt.plot( b, label = \"Cauchy\" )\n",
    "plt.plot( c, label = \"Exponential\" )\n",
    "plt.plot( d, label = \"Exponential + Cauchy\" )\n",
    "\n",
    "plt.legend( loc = \"best\")\n",
    "plt.xlabel( '#observations')\n",
    "plt.ylabel( 'r.v value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2796a",
   "metadata": {},
   "source": [
    "#### 2- Create a discrete uniformly distributed (values between 0 and 10) and Bernoulli ($p=0.5$) random variable ( r.v ) vectors of length $n=10^5$. First, plot each r.v separately in a histogram. Then sum up different amounts of r.vs of the same kind ( Bernoulli + Bernoulli + ...) , and plot their sums into a histogram. What can you observe?\n",
    "\n",
    "Note: be sure to declare the r.vs into their own variables.\n",
    "Example:\n",
    "uniform_1 = stats.randint( 0, 10 );\n",
    "uniform_2 = stats.randint( 0, 10 )\n",
    "\n",
    "plt.hist( uniform_1 + uniform_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02229f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Urv = stats.randint( 0, 10 ) # Discrete uniform distributed r.v between 0 and 10\n",
    "\n",
    "a = Urv.rvs( size = 10**5 )\n",
    "b = Urv.rvs( size = 10**5 )\n",
    "c = Urv.rvs( size = 10**5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist( a+b+c );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb849079",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brv = stats.bernoulli( p = 0.5 )\n",
    "\n",
    "b0 = Brv.rvs( size = 10**5 )\n",
    "b2 = Brv.rvs( size = 10**5 )\n",
    "b1 = Brv.rvs( size = 10**5 )\n",
    "b3 = Brv.rvs( size = 10**5 )\n",
    "b4 = Brv.rvs( size = 10**5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde730f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist( b0 + b1 + b2 + b3 + b4 );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab0e52",
   "metadata": {},
   "source": [
    "#### 3- Create two vectors of Gaussian random variables ( r.vs ) and perform a linear regression, using scipy.stats.linregress. Perform the regression, for example, $k=1000$  times and plot the histogram of correlation coffecients and p-values.\n",
    "\n",
    "#### Bonus 1: experiment the same linear regression with other r.vs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = []\n",
    "P = []\n",
    "k = 100\n",
    "n = 200000\n",
    "for k in range( k ):\n",
    "\n",
    "    X = norm.rvs( size = n, random_state = rng )\n",
    "    Y = expon.rvs( size = n, random_state = rng )\n",
    "   \n",
    "    slope, intercept, r, p, se = stats.linregress(X, Y)\n",
    "    C.append( r )\n",
    "    P.append( p )\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize = ( 10, 7 ))\n",
    "axs[0].hist( C )\n",
    "axs[0].set_xlabel('Correlation')\n",
    "axs[0].set_ylabel('Freq')\n",
    "axs[0].set_title( 'Distribution of Pearson correlation')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].hist( P  )\n",
    "axs[1].set_ylabel('Freq')\n",
    "axs[1].set_xlabel('P-value')\n",
    "axs[1].set_title( 'Distribution of P-values from Linear regression')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5dcce",
   "metadata": {},
   "source": [
    "#### 3-  Run each model, on the Iris data set, $rounds=k$ times using different train/test partitions and take the avegarge of their error rates. Plot and compare the models below using 0-1 metric on the predicted data and labelled data. Experiment with different number of $k$ rounds.\n",
    "\n",
    "+ Use default definitions of the models.\n",
    "+ For plotting: use train set proportion on the x-axis and probability of error on the y-axis\n",
    "\n",
    "#### Bonus 1: experiment with other model loss functions, penalites, and solvers. For example, loss = \"squared_hinge\" SGDClassifier. Use the help() function to see what options are available in a particular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "iris = data.load_iris()\n",
    "X    = iris.data\n",
    "y    = iris.target\n",
    "\n",
    "heldout = [ 0.90, 0.75, 0.50, 0.02 ] # suggested starting point. Modify to your liking.\n",
    "rounds  = 1\n",
    "\n",
    "models = [\n",
    "    (\"SGD\", SGDClassifier(max_iter=110)),\n",
    "    (\"Perceptron\", Perceptron(max_iter=110)),\n",
    "    (\n",
    "        \"Passive-Aggressive I\",\n",
    "        PassiveAggressiveClassifier(max_iter=110, loss=\"hinge\", C=1.0, tol=1e-4),\n",
    "    ),\n",
    "    (\n",
    "        \"Passive-Aggressive II\",\n",
    "        PassiveAggressiveClassifier( max_iter=110, loss=\"squared_hinge\", C=1.0, tol=1e-4 ),\n",
    "    ),\n",
    "    (\n",
    "        \"SAG\",\n",
    "        LogisticRegression(max_iter=110, solver=\"sag\", tol=1e-1, C=1.0e4 / X.shape[0]),\n",
    "    ),\n",
    "]\n",
    "\n",
    "xx = 1.0 - np.array(heldout)\n",
    "\n",
    "plt.figure( figsize=( 10, 7 ) )\n",
    "\n",
    "for name, clf in models:\n",
    "    print( f\"training {name}\")\n",
    "    yy = []\n",
    "    for i in heldout:\n",
    "        yy_ = []\n",
    "        for r in range(rounds):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=i, random_state = rng.integers( 124 )\n",
    "            )\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            yy_.append( metrics.zero_one_loss( y_test, y_pred, normalize = False ) )\n",
    "        yy.append(np.mean(yy_))\n",
    "    plt.plot(xx, yy, label=name)\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Proportion of training data(%)\" )\n",
    "plt.ylabel(\"0-1 loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc51b45",
   "metadata": {},
   "source": [
    "#### 4a- Repeat the experiments on task 4, but on a single train/test split, for each model separately and plot the model's error rates in a histogram.\n",
    "\n",
    "##### Bonus: use other loss functions from sklearn.metrics for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(name, clf) = models[ 0 ]\n",
    "\n",
    "yy = []\n",
    "    \n",
    "yy_ = []\n",
    "for r in range( 5000 ):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = rng.integers( 124 ) )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    yy_.append( metrics.zero_one_loss( y_test, y_pred ) )\n",
    "\n",
    "plt.figure( figsize=( 10, 7 ) )\n",
    "plt.hist( yy_, bins = np.unique( yy_ )  )\n",
    "plt.title(\"SGD - Bar of 0-1 loss with 80/20 split\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aaa849",
   "metadata": {},
   "source": [
    "#### 4b- Choose an accuracy from the previous exercise (4a), and compute a Binomial distribution of how would you expect the accuracy perform if that specific accuarcy would be expected. Insert $p$ as the accuracy obtained from the model into $\\binom{n}{k}p^k(1-p)^{n-k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cfc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "n, p = 100000, 0.9\n",
    "\n",
    "mean, var, skew, kurt = binom.stats(n, p, moments='mvsk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( mean, var, skew, kurt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eaae59-0b82-4a03-a3de-9b4da2d898cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(binom.ppf(0.01, n, p),binom.ppf(0.99, n, p))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\n",
    "ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\n",
    "\n",
    "rv = binom(n, p)\n",
    "ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,label='frozen pmf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a241c46-a919-458d-a02c-0e91f4d934ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
