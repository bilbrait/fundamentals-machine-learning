{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.datasets as data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' ) # use whitegrid if prefer a white background\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#matplotlib.rcParams.update( { 'font.size': 18 } ) # Use this to setup your preferred font size for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbd7aa",
   "metadata": {},
   "source": [
    "#### 1- Use Self-Organizing map to see how a random set of colors self-organize with different learning rates,  number of epochs, grid sizes and training set sizes.\n",
    "+ (a) Define learning rate as exp( $-t\\gamma$ ), where $\\gamma$ is the decay rate, and $t$ is a range of epochs.\n",
    "+ (b) Define neighborhood distance between neurons on a grid, as exp( $d^2 / (2\\sigma^2))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4bfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the learning rate behave with different number of epochs and decay rates\n",
    "# \n",
    "epochs = np.arange(0, ? )\n",
    "lr_decay = [ ?, ?, ?, ? ]\n",
    "fig,ax = plt.subplots(nrows=1, ncols = 4, figsize=(15,4))\n",
    "plt_ind = np.arange(4) + 141\n",
    "for decay, ind in zip(lr_decay, plt_ind):\n",
    "    plt.subplot(ind)\n",
    "    learn_rate =  # (a)\n",
    "    plt.plot(epochs, learn_rate, c='red')\n",
    "    plt.title('decay rate: ' + str(decay))\n",
    "    plt.xlabel('epochs $t$')\n",
    "    plt.ylabel('$\\eta^(t)$')\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fef842",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.arange(0, 10)\n",
    "sigma_sq = [ ?, ?, ?, ? ] # Try out different spread (variance) ranges\n",
    "fig,ax = plt.subplots(nrows=1, ncols=4, figsize=(15,4))\n",
    "plt_ind = np.arange(4) + 141\n",
    "for s, ind in zip(sigma_sq, plt_ind):\n",
    "    plt.subplot(ind)\n",
    "    f =  # (b)\n",
    "    plt.plot(distance, f, c='red')\n",
    "    plt.title('$\\sigma^2$ = ' + str(s))\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Neighborhood function $f$')\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd738f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the (g,h) index of the BMU in the grid\n",
    "def find_BMU(SOM,x):\n",
    "    distSq = (np.square(SOM - x)).sum(axis=2)\n",
    "    return np.unravel_index(np.argmin(distSq, axis=None), distSq.shape)\n",
    "    \n",
    "# Update the weights of the SOM cells when given a single training example\n",
    "# and the model parameters along with BMU coordinates as a tuple\n",
    "def update_weights(SOM, train_ex, learn_rate, radius_sq, \n",
    "                   BMU_coord, step=3):\n",
    "    g, h = BMU_coord\n",
    "    #if radius is close to zero then only BMU is changed\n",
    "    if radius_sq < 1e-3:\n",
    "        SOM[g,h,:] += learn_rate * (train_ex - SOM[g,h,:])\n",
    "        return SOM\n",
    "    # Change all cells in a small neighborhood of BMU\n",
    "    for i in range(max(0, g-step), min(SOM.shape[0], g+step)):\n",
    "        for j in range(max(0, h-step), min(SOM.shape[1], h+step)):\n",
    "            dist_sq = np.square(i - g) + np.square(j - h)\n",
    "            dist_func = np.exp(-dist_sq / 2 / radius_sq)\n",
    "            SOM[i,j,:] += learn_rate * dist_func * (train_ex - SOM[i,j,:])   \n",
    "    return SOM    \n",
    "\n",
    "# Main routine for training an SOM. It requires an initialized SOM grid\n",
    "# or a partially trained grid as parameter\n",
    "def train_SOM(SOM, train_data, learn_rate = .1, radius_sq = 1, \n",
    "             lr_decay = .1, radius_decay = .1, epochs = 10):    \n",
    "    learn_rate_0 = learn_rate\n",
    "    radius_0 = radius_sq\n",
    "    for epoch in np.arange(0, epochs):\n",
    "        rand.shuffle(train_data)      \n",
    "        for train_ex in train_data:\n",
    "            g, h = find_BMU(SOM, train_ex)\n",
    "            SOM = update_weights(SOM, train_ex, \n",
    "                                 learn_rate, radius_sq, (g,h))\n",
    "        # Update learning rate and radius\n",
    "        learn_rate = learn_rate_0 * np.exp(-epoch * lr_decay)\n",
    "        radius_sq = radius_0 * np.exp(-epoch * radius_decay)            \n",
    "    return SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29222d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of the SOM grid\n",
    "m = 5 # Try out different grid sizes\n",
    "n = 5 \n",
    "# Number of training examples\n",
    "n_x = 3000 # Try out different number of training samples\n",
    "rand = np.random.RandomState(0)\n",
    "# Initialize the training data\n",
    "train_data = rand.randint(0, 255, (n_x, 3))\n",
    "# Initialize the SOM randomly\n",
    "SOM = rand.randint(0, 255, (m, n, 3)).astype(float)\n",
    "# Display both the training matrix and the SOM grid\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=1, ncols=2, figsize=(12, 3.5), \n",
    "    subplot_kw=dict(xticks=[], yticks=[]))\n",
    "ax[0].imshow(train_data.reshape(50, 60, 3))\n",
    "ax[0].title.set_text('Training Data')\n",
    "ax[1].imshow(SOM.astype(int))\n",
    "ax[1].title.set_text('Randomly Initialized SOM Grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    nrows=1, ncols=5, figsize=(15, 3.5), \n",
    "    subplot_kw=dict(xticks=[], yticks=[]))\n",
    "total_epochs = 0\n",
    "for epochs, i in zip([ ?, ?, ?, ? ], range(0,5)):\n",
    "    total_epochs += epochs\n",
    "    SOM = train_SOM(SOM, train_data, epochs=epochs)\n",
    "    ax[i].imshow(SOM.astype(int))\n",
    "    ax[i].title.set_text('Epochs = ' + str(total_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8527839",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    nrows=3, ncols=3, figsize=(15, 15), \n",
    "    subplot_kw=dict(xticks=[], yticks=[]))\n",
    "\n",
    "# Initialize the SOM randomly to the same state\n",
    "\n",
    "for learn_rate, i in zip([0.001, 0.5, 0.99], [0, 1, 2]):\n",
    "    for radius_sq, j in zip([0.01, 1, 10], [0, 1, 2]):\n",
    "        rand = np.random.RandomState(0)\n",
    "        SOM = rand.randint(0, 255, (m, n, 3)).astype(float)        \n",
    "        SOM = train_SOM(SOM, train_data, epochs = 5,\n",
    "                        learn_rate = learn_rate, \n",
    "                        radius_sq = radius_sq)\n",
    "        ax[i][j].imshow(SOM.astype(int))\n",
    "        ax[i][j].title.set_text('$\\eta$ = ' + str(learn_rate) + \n",
    "                                ', $\\sigma^2$ = ' + str(radius_sq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302667a",
   "metadata": {},
   "source": [
    "#### 2- Compare Principal Component Analysis (PCA), Kernel PCA, Singular Value Decomposition and Linear Discriminant Analysis (LDA) in dimensionality reduction.\n",
    "+ PCA: finds the maximum variation between variables in terms of variance (spread from the mean)\n",
    "+ LDA: finds the maximum separation in terms of the ratio of between and in-class variances.\n",
    "+ SVD: finds the eigenvalue and eigenvector representation of a data matrix\n",
    "+ Kernel PCA: Kernelized version of PCA\n",
    "+ Try adding Gaussian noise to the observations and observe how sensitive the projections are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components = ? )\n",
    "X_r2 = lda.fit(X, y).transform(X)\n",
    "\n",
    "svd = TruncatedSVD( n_components = ? )\n",
    "Xsvd = svd.fit( X, y ).transform( X )\n",
    "\n",
    "kpca = KernelPCA( n_components = ? )\n",
    "Xkpca = kpca.fit( X, y ).transform( X )\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print(\n",
    "    \"explained PCA variance ratio (first two components): %s\"\n",
    "    % str(pca.explained_variance_ratio_)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"explained SVD variance ratio (first two components): %s\"\n",
    "    % str(svd.explained_variance_ratio_)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "colors = [\"navy\", \"turquoise\", \"darkorange\"]\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(\n",
    "        X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\n",
    "    )\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.title(\"PCA of IRIS dataset\")\n",
    "\n",
    "plt.figure()\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(\n",
    "        X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name\n",
    "    )\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.title(\"LDA of IRIS dataset\")\n",
    "\n",
    "plt.figure()\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(\n",
    "        Xsvd[y == i, 0], Xsvd[y == i, 1], alpha=0.8, color=color, label=target_name\n",
    "    )\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.title(\"SVD of IRIS dataset\")\n",
    "\n",
    "plt.figure()\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(\n",
    "        Xkpca[y == i, 0], Xsvd[y == i, 1], alpha=0.8, color=color, label=target_name\n",
    "    )\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.title(\"Kernel PCA of IRIS dataset\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33b92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f8f5f1b",
   "metadata": {},
   "source": [
    "#### 3- Use PCA, Kernel PCA, t-SNE and Truncated SVD to create an embedding for the MNIST dataset. Adjust the model parameters and see how the embedding changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf903f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = data.load_digits()\n",
    "X, y   = data.load_digits( return_X_y = True )\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors           = ?\n",
    "\n",
    "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\n",
    "    ax.axis(\"off\")\n",
    "_ = fig.suptitle(\"A selection from the 64-dimensional digits dataset\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7523e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "from fml5_helper_funcs import plot_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ba4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = { \"Truncated SVD embedding\": TruncatedSVD( n_components = ? ),\n",
    "               \"MDS embedding\"          : MDS( n_components = ?, n_init=?, max_iter=?, n_jobs=2),\n",
    "               \"PCA\"                    : PCA( n_components = ? ),\n",
    "               \"KPCA\"                   : KernelPCA( n_components = ? )\n",
    "            ,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ec88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "projections =  {}\n",
    "for name, transformer in embeddings.items():\n",
    "    print(f\"Computing {name}...\")\n",
    "\n",
    "    projections[name] = transformer.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e81b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in projections:\n",
    "    title = f\"{name}\"\n",
    "    plot_embedding(projections[name], y, digits, title)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
