{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88343a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.datasets as data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' ) # use whitegrid if prefer a white background\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#matplotlib.rcParams.update( { 'font.size': 18 } ) # Use this to setup your preferred font size for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbd7aa",
   "metadata": {},
   "source": [
    "#### 1- How sensitive PCA and ICA are to the number of observations? What changes the most in both PCA and ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "np_rng = np.random.RandomState( 422 )\n",
    "S      = np_rng.standard_t(1.5, size=(2000, 2)) # Vary the number of points: 20000, 20001, .... \n",
    "S[:, 0] *= 2.\n",
    "\n",
    "# Mix data\n",
    "A = np.array([[1, 1], [0, 2]])  # Mixing matrix\n",
    "\n",
    "# Generate observations by computing the matrix product SA^T\n",
    "X = ?  \n",
    "\n",
    "pca    = PCA()\n",
    "S_pca_ = pca.fit(X).transform(X)\n",
    "\n",
    "ica    = FastICA(random_state=rng.integers( 42 ) )\n",
    "S_ica_ = ica.fit(X).transform(X)  # Estimate the sources\n",
    "\n",
    "# Scale row-wise using standard deviation of S_ica_\n",
    "S_ica_ /= S_ica_.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4bfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(S, axis_list=None):\n",
    "    # Plot\n",
    "    \n",
    "    if axis_list is not None:\n",
    "        colors = ['orange', 'red']\n",
    "        for color, axis in zip(colors, axis_list):\n",
    "            axis /= axis.std()\n",
    "            x_axis, y_axis = axis\n",
    "            # Trick to get legend to work\n",
    "            plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color)\n",
    "            plt.quiver((0, 0), (0, 0), x_axis, y_axis, zorder=11, width=0.01,\n",
    "                       scale=6, color=color)\n",
    "\n",
    "    plt.hlines(0, -3, 3)\n",
    "    plt.vlines(0, -3, 3)\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 14, 9 ) )\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_samples(S / S.std())\n",
    "plt.title('True Independent Sources')\n",
    "\n",
    "# PCA components and ICA mixing components\n",
    "axis_list = [?_.T, ?]\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_samples(X / np.std(X), axis_list=axis_list)\n",
    "legend = plt.legend(['PCA', 'ICA'], loc='upper right')\n",
    "legend.set_zorder(100)\n",
    "\n",
    "plt.title('Observations')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_samples(S_pca_ / np.std(S_pca_, axis=0))\n",
    "plt.title('PCA recovered signals')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_samples(S_ica_ / np.std(S_ica_))\n",
    "plt.title('ICA recovered signals')\n",
    "\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302667a",
   "metadata": {},
   "source": [
    "#### 2- Use Non negative matrix factorization and Latent Dirichlet Allocation to group a collection of words into set topics (that is clusters). Experiment with different hyperparameters and also different number of features, sample sizes, top words and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from fml5_helper_funcs import plot_top_words\n",
    "\n",
    "newsgroup, _ = fetch_20newsgroups( shuffle=True, random_state = rng.integers( 321 ), \n",
    "                                   remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "                                   return_X_y=True,\n",
    "                                 )\n",
    "n_samples    = 2000\n",
    "n_features   = 1000\n",
    "n_components = 10\n",
    "n_top_words  = 20\n",
    "batch_size   = 128\n",
    "init         = \"nndsvda\"\n",
    "news_samples = ?\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer( max_df  =0.95, min_df=2, max_features=n_features, stop_words=\"english\" )\n",
    "tfidf            = ?\n",
    "\n",
    "nmf = ?\n",
    "\n",
    "tfidf_feature_names_nmf = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "plot_top_words( nmf, tfidf_feature_names_nmf, n_top_words, \"Topics in NMF model (Frobenius norm)\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = ?\n",
    "lda.fit( tfidf )\n",
    "\n",
    "tfidf_feature_names_lda = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "plot_top_words(lda, tfidf_feature_names_lda, n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f5f1b",
   "metadata": {},
   "source": [
    "#### 3- Use PCA, Multidimensional scaling, and Truncated SVD to create an embedding for the MNIST dataset. Experiment with different hyperparameters to see how the they affect the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf903f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = data.load_digits()\n",
    "X, y   = data.load_digits( return_X_y = True )\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
    "for idx, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)\n",
    "    ax.axis(\"off\")\n",
    "_ = fig.suptitle(\"A selection from the 64-dimensional digits dataset\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7523e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.manifold import MDS\n",
    "from fml5_helper_funcs import plot_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ba4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = { ? }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ec88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "projections =  {}\n",
    "for name, transformer in embeddings.items():\n",
    "    print(f\"Computing {name}...\")\n",
    "\n",
    "    ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e81b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in projections:\n",
    "    title = f\"{name}\"\n",
    "    plot_embedding(projections[name], y, digits, title)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
