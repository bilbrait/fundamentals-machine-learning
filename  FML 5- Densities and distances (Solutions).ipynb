{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36dacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.datasets as data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' ) # use whitegrid if prefer a white background\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#matplotlib.rcParams.update( { 'font.size': 18 } ) # Use this to setup your preferred font size for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18a92a",
   "metadata": {},
   "source": [
    "#### 1- First estimate different covariances on the Iris dataset using Gaussian Mixture Models. Plot the different covriance estimation and how well they fit to the data with different number of components and iterations.\n",
    "\n",
    "+ Covariance is defined as the dispersion of observation pairs in terms how much they differ from a mean value (class specific in case). Notationally, covariance is defined as: mean( $x_i$ - mean($x_i$) ) * mean( $x_j$ - mean($x_j$ ) )\n",
    "+ Try different initializations for the centroids. By default, K-means clustering is used.\n",
    "+ Plot the class-specific covariance matrices to check which Gaussian mixture estimator makes sense. \n",
    "+ use np.random.norma( mu, sigma, X.shape ) and see how noise affects the estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaafefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from fml5_helper_funcs import make_ellipses\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0461aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data.load_iris( return_X_y = True )\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split( X, Y, \n",
    "                                                 test_size = 0.3,\n",
    "                                                 random_state = rng.integers( 4231 ) \n",
    "                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7acf51",
   "metadata": {},
   "source": [
    "+ Spherical covariance: each mixture component has its own (single) covariance\n",
    "+ Diagonal covariance: each mixture component has the covariance in the diagonal\n",
    "+ Tied covariance: each mixture component has the same covariance\n",
    "+ Full covariance: each mixture componen has a general definition of covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different number of components in the Gaussian Mixture\n",
    "# model and different amount of iterations\n",
    "\n",
    "GMMs = {cov_type: GaussianMixture( n_components    = 3       ,\n",
    "                                   covariance_type = cov_type, \n",
    "                                   max_iter        =20       ,\n",
    "                                   random_state = rng.integers( 212 )\n",
    "                                 )\n",
    "              for cov_type in ['spherical', 'diag', 'tied', 'full']\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2770b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes    = 3\n",
    "n_estimators = len( GMMs )\n",
    "ynames = [ 'Setosa', 'Versicolour', 'Virginica']\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "plt.figure(figsize=(3 * n_estimators // 2, 6))\n",
    "plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n",
    "                    left=.01, right=.99)\n",
    "\n",
    "\n",
    "for index, ( name, estimator ) in enumerate(GMMs.items()):\n",
    "    # Since we have class labels for the training data, we can\n",
    "    # initialize the GMM parameters in a supervised manner.\n",
    "    estimator.means_init = np.array([Xtrain[ytrain == i].mean(axis=0)\n",
    "                                    for i in range(n_classes)])\n",
    "\n",
    "    # Train the other parameters using the EM algorithm.\n",
    "    estimator.fit(Xtrain)\n",
    "    h = plt.subplot(2, n_estimators // 2, index + 1)\n",
    "    make_ellipses(estimator, h, colors )\n",
    "\n",
    "    for n, color in enumerate(colors):\n",
    "        data = X[ Y == n ]\n",
    "        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n",
    "                    label=ynames[n])\n",
    "    # Plot the test data with crosses\n",
    "    for n, color in enumerate(colors):\n",
    "        data = Xtest[ytest == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n",
    "\n",
    "    y_train_pred = estimator.predict(Xtrain)\n",
    "    train_accuracy = np.mean(y_train_pred.ravel() == ytrain.ravel()) * 100\n",
    "    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,\n",
    "             transform=h.transAxes)\n",
    "\n",
    "    y_test_pred = estimator.predict(Xtest)\n",
    "    test_accuracy = np.mean(y_test_pred.ravel() == ytest.ravel()) * 100\n",
    "    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,\n",
    "             transform=h.transAxes)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(name)\n",
    "\n",
    "plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_cov = np.cov( X[ 0:50 ] ) # Example for the Setosa class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb49a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow( X0_cov ); plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24acddb6",
   "metadata": {},
   "source": [
    "#### 2- Perform K-means clustering on the Iris dataset. Try adding Gaussian noise, different $\\mu$ and $\\sigma$, to the observations and see how the noise and covariance structure of the data affects the computing of the clusters. How does this compare to the Gaussian Mixture Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdba02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    \n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    clusterer      = KMeans(n_clusters=n_clusters, random_state = rng.integers( 2415 ) )\n",
    "    X              = X + np.random.normal( 0, 1, X.shape )\n",
    "    cluster_labels = clusterer.fit_predict( X )\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score( X, cluster_labels )\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range( n_clusters ):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed. See how the clusters look from different feature columns\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18b73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b277842",
   "metadata": {},
   "source": [
    "#### 3- Use Nearest neighbors to do a paramteric clustering on the Iris data set.\n",
    "+ Try out different number of neighbors. Use the same variables as in the two previous exercises so that you can compare the results qualitatively.\n",
    "+ Shrinkage divides each variable within a class by its variance (deviation from the mean). Experiment with different variance levels to see how groupings change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "n_neighbors = 15\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "cmap_bold = ListedColormap([\"darkorange\", \"c\", \"darkblue\"])\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split( X, Y, \n",
    "                                                 test_size = 0.3,\n",
    "                                                 random_state = rng.integers( 4231 ) \n",
    "                                               )\n",
    "\n",
    "#  \n",
    "for shrinkage in [None, 0.2, 0.4, 0.9]:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = NearestCentroid(shrink_threshold=shrinkage)\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "    y_pred = clf.predict(Xtest)\n",
    "    print(shrinkage, np.mean(ytest == y_pred))\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf, X, cmap=cmap_light, ax=ax, response_method=\"predict\"\n",
    "    )\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n",
    "    plt.title(\"3-Class classification (shrink_threshold=%r)\" % shrinkage)\n",
    "    plt.axis(\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14d994",
   "metadata": {},
   "source": [
    "#### 4- Experiment Agglomerative clustering on a simulated data.\n",
    "+ Try out different number of clusters ( clusterA, clusterB variables ) and different number of sample points ( n_samples ). How does the clustering behaves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# Generate sample data\n",
    "n_samples = 1500\n",
    "np.random.seed(0)\n",
    "t = 1.5 * np.pi * (1 + 3 * np.random.rand(1, n_samples))\n",
    "x = t * np.cos(t)\n",
    "y = t * np.sin(t)\n",
    "\n",
    "\n",
    "X = np.concatenate((x, y))\n",
    "X += 0.7 * np.random.randn(2, n_samples)\n",
    "X = X.T\n",
    "\n",
    "clusterA = 30\n",
    "clusterB = 3\n",
    "\n",
    "knn_graph = kneighbors_graph(X, 30, include_self=False)\n",
    "\n",
    "for connectivity in (None, knn_graph):\n",
    "    # Experiment with different number of clusters\n",
    "    for n_clusters in (clusterA, clusterB):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        for index, linkage in enumerate((\"average\", \"complete\", \"ward\", \"single\")):\n",
    "            plt.subplot(1, 4, index + 1)\n",
    "            model = AgglomerativeClustering(\n",
    "                linkage=linkage, connectivity=connectivity, n_clusters=n_clusters\n",
    "            )\n",
    "            \n",
    "            model.fit(X)\n",
    "            \n",
    "            plt.scatter(X[:, 0], X[:, 1], c=model.labels_, cmap=plt.cm.nipy_spectral)\n",
    "            plt.title(\n",
    "                \"linkage=%s\" % (linkage),\n",
    "                fontdict=dict(verticalalignment=\"top\"),\n",
    "            )\n",
    "            plt.axis(\"equal\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplots_adjust(bottom=0, top=0.83, wspace=0, left=0, right=1)\n",
    "            plt.suptitle(\n",
    "                \"n_cluster=%i, connectivity=%r\"\n",
    "                % (n_clusters, connectivity is not None),\n",
    "                size=17,\n",
    "            )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
