{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c322662",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.datasets as data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' ) # use whitegrid if prefer a white background\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#matplotlib.rcParams.update( { 'font.size': 18 } ) # Use this to setup your preferred font size for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X, y = data.load_breast_cancer( return_X_y = True, as_frame = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6502bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help( data.load_breast_cancer )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e55f37",
   "metadata": {},
   "source": [
    "#### 1- Compare Gradient Boosting, SVM and Multi-layer Perceptrons using Cross validation with Receiver Characteristic Operator curves, and confusion matrix on the provided dataset above. That is,\n",
    "\n",
    "+ Examine how regularization affects the training and cross-validation curves\n",
    "+ Examine how different K-fold evaluations affects model fitting using the ROC curves\n",
    "+ Select the best regulairzation parameter and then confusion matrices of the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Learning curves\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Cross-validation and ROC curves\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc101346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_cv_roc( model, Kfolds, X, y, model_name ):\n",
    "    '''\n",
    "    Plots the ROC curves of each K-fold.\n",
    "    model <- used estimator\n",
    "    Kfolds <- Number of K-folds\n",
    "    X <- feature data\n",
    "    y <- label data\n",
    "    model_name <- string containing model name for plot title\n",
    "    '''\n",
    "    K  = Kfolds\n",
    "    cv = StratifiedKFold( n_splits = K )\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    fig, ax = plt.subplots( figsize = ( 10, 8 ))\n",
    "    for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        model.fit(X[train], y[train])\n",
    "        viz = RocCurveDisplay.from_estimator(model, X[test], y[test],\n",
    "                             name='ROC fold {}'.format(i),\n",
    "                             alpha=0.3, lw=1, ax=ax)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "            label='Chance', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "           title=\"{0} Receiver operating characteristic example\".format( model_name ))\n",
    "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433c596",
   "metadata": {},
   "source": [
    "##### SVM: Using the Radial basis function $\\text{ exp}(-\\gamma||x_i - x_j||^2)$,  vary the $\\gamma$ parameter which scales the size of the kernel. Also experiment with different regularization C terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC( C = 0.5, kernel = 'rbf' )\n",
    "\n",
    "param_range = np.logspace(?, ?, ?)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    svc, X, y, param_name=\"gamma\", param_range=param_range,\n",
    "    scoring=\"accuracy\", n_jobs=4)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure( figsize = ( 10, 7 ) )\n",
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(r\"Margin separation range\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.4, 1.2)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_cv_roc( svc, ?, X, y, 'SVM' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db66de4",
   "metadata": {},
   "source": [
    "###### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6995d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier( hidden_layer_sizes = (?, ?) ) # adding more hidden layers: ( a, b, c, ...)\n",
    "\n",
    "param_range = np.logspace(?, ?, ?)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    mlp, X, y, param_name=\"alpha\", param_range=param_range,\n",
    "    scoring=\"accuracy\", n_jobs=4)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure( figsize = ( 10, 7 ) )\n",
    "plt.title(\"Validation Curve with MLP\")\n",
    "plt.xlabel(r\"Parameter range\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.4, 1.2)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_cv_roc( mlp, ?, X, y, 'MLP' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18d2fa",
   "metadata": {},
   "source": [
    "###### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "param_range = np.arange( ?, ? )\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    gb, X, y, param_name=\"n_estimators\", param_range=param_range,\n",
    "    scoring=\"accuracy\", n_jobs=4)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure( figsize = ( 10, 7 ) )\n",
    "plt.title(\"Validation Curve with Gradient boosting\")\n",
    "plt.xlabel(r\"Parameter range\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.4, 1.2)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2972d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_cv_roc( gb, ?, X, y, 'Gradient boosting' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea3c0b",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix( y, mlp.predict( X ) )\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix( y, svc.predict( X ) )\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix( y, gb.predict( X ) )\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f27f82",
   "metadata": {},
   "source": [
    "#### 2- Use Grid search for searching hyperparamters for the Multi-layered Perceptron. Fix some other hyperparamters by guessing but use Grid search to select the best solver, activation function, and number of hidden layers\n",
    "+ Bonus: Do the same with SVM and Gradient boosting if you have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.load_digits( return_X_y = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79adb43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# You may choose a different test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.5, random_state = rng.integers( 432 ) )\n",
    "\n",
    "tuned_parameters = [{'solver': ['sgd', 'adam', 'lbfgs'], \n",
    "                     'activation': ['relu', 'logistic'], \n",
    "                     'hidden_layer_sizes': [ (10, ), ( 15, 15 ), (18, 10 )]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        MLPClassifier( max_iter = 5), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc102100",
   "metadata": {},
   "source": [
    "#### 3- Use MLP, SVM or Gradient boosting to do a recursive feature selection on the breast cancer dataset.\n",
    "+ Bonus: If time, use the approach in Task 2, then move perform a recursive feature elimination with the hyperparameter optimized model. Is this a valid approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV # use help to check RFECV\n",
    "\n",
    "X, y = data.load_breast_cancer( return_X_y = True, as_frame = True )\n",
    "print( X.shape )\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3726333",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc                 = SVC( kernel = 'linear')\n",
    "Kfold               = 5\n",
    "select_min_features = 6\n",
    "\n",
    "rfecv = RFECV(estimator=svc, step = 1, \n",
    "              cv = StratifiedKFold( Kfold ),\n",
    "              scoring='accuracy',\n",
    "              min_features_to_select = 5 )\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "\n",
    "n_scores = len(rfecv.cv_results_[\"mean_test_score\"])\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean test accuracy\")\n",
    "plt.errorbar(\n",
    "    range(select_min_features, n_scores + select_min_features),\n",
    "    rfecv.cv_results_[\"mean_test_score\"],\n",
    "    yerr=rfecv.cv_results_[\"std_test_score\"],\n",
    ")\n",
    "plt.title(\"Recursive Feature Elimination\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460ae23",
   "metadata": {},
   "source": [
    "#### 4- This an advance approach to model selection based on Bayesian Analysis. If you feel bored, then proceed to do this. Use whatever model you wish. This solution example is done using a MLP model.\n",
    "\n",
    "#### Use the approach in Task 2 to hyperparameter optimize a selected model. Use either the same set of candidate parameters or make up more.\n",
    "\n",
    "+ To understan the Bayes Analysis -based comparison of models, read on Region of Practical Equivalence https://doingbayesiandataanalysis.blogspot.com/2013/08/how-much-of-bayesian-posterior.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad346ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# recommend using a small number of candidates to make sense of some plots\n",
    "tuned_parameters = [{'solver': ['sgd', 'adam', 'lbfgs'], \n",
    "                     'activation': ['relu', 'logistic'], \n",
    "                     'hidden_layer_sizes': [10, 15, 18]}]\n",
    "\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=2, n_repeats=2, random_state=rng.integers( 5324 )\n",
    ")\n",
    "\n",
    "mlp = MLPClassifier( max_iter = 5)\n",
    "\n",
    "search = GridSearchCV( estimator = mlp, param_grid=tuned_parameters, scoring='roc_auc', cv=cv )\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fbb9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(search.cv_results_)\n",
    "results_df = results_df.sort_values(by=['rank_test_score'])\n",
    "results_df = (\n",
    "    results_df\n",
    "    .set_index(results_df[\"params\"].apply(\n",
    "        lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "    )\n",
    "    .rename_axis('kernel')\n",
    ")\n",
    "results_df[\n",
    "    ['params', 'rank_test_score', 'mean_test_score', 'std_test_score']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5592e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create df of model scores ordered by perfomance\n",
    "model_scores = results_df.filter(regex=r'split\\d*_test_score')\n",
    "\n",
    "# plot 30 examples of dependency between cv fold and AUC scores\n",
    "fig, ax = plt.subplots()\n",
    "sb.lineplot(\n",
    "    data=model_scores.transpose().iloc[:30],\n",
    "    dashes=False, palette='Set1', marker='o', alpha=.5, ax=ax\n",
    ")\n",
    "ax.set_xlabel(\"CV test fold\", size=12, labelpad=10)\n",
    "ax.set_ylabel(\"Model AUC\", size=12)\n",
    "ax.tick_params(bottom=True, labelbottom=False)\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "#plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# print correlation of AUC scores across folds\n",
    "print(f\"Correlation of models:\\n {model_scores.transpose().corr()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "def corrected_std(differences, n_train, n_test):\n",
    "    \"\"\"Corrects standard deviation using Nadeau and Bengio's approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    differences : ndarray of shape (n_samples, 1)\n",
    "        Vector containing the differences in the score metrics of two models.\n",
    "    n_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_test : int\n",
    "        Number of samples in the testing set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corrected_std : int\n",
    "        Variance-corrected standard deviation of the set of differences.\n",
    "    \"\"\"\n",
    "    n = n_train + n_test\n",
    "    corrected_var = (\n",
    "        np.var(differences, ddof=1) * ((1 / n) + (n_test / n_train))\n",
    "    )\n",
    "    corrected_std = np.sqrt(corrected_var)\n",
    "    return corrected_std\n",
    "\n",
    "\n",
    "def compute_corrected_ttest(differences, df, n_train, n_test):\n",
    "    \"\"\"Computes right-tailed paired t-test with corrected variance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    differences : array-like of shape (n_samples, 1)\n",
    "        Vector containing the differences in the score metrics of two models.\n",
    "    df : int\n",
    "        Degrees of freedom.\n",
    "    n_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_test : int\n",
    "        Number of samples in the testing set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    t_stat : float\n",
    "        Variance-corrected t-statistic.\n",
    "    p_val : float\n",
    "        Variance-corrected p-value.\n",
    "    \"\"\"\n",
    "    mean = np.mean(differences)\n",
    "    std = corrected_std(differences, n_train, n_test)\n",
    "    t_stat = mean / std\n",
    "    p_val = t.sf(np.abs(t_stat), df)  # right-tailed t-test\n",
    "    return t_stat, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_scores = model_scores.iloc[0].values  # scores of the best model\n",
    "model_2_scores = model_scores.iloc[1].values  # scores of the second-best model\n",
    "\n",
    "differences = model_1_scores - model_2_scores\n",
    "\n",
    "n       = differences.shape[0]  # number of test sets\n",
    "df      = n - 1\n",
    "n_train = len(list(cv.split(X, y))[0][0])\n",
    "n_test  = len(list(cv.split(X, y))[0][1])\n",
    "\n",
    "# intitialize random variable\n",
    "t_post = t(\n",
    "    df, loc=np.mean(differences),\n",
    "    scale=corrected_std(differences, n_train, n_test)\n",
    ")\n",
    "\n",
    "x = np.linspace(t_post.ppf(0.001), t_post.ppf(0.999), 100)\n",
    "\n",
    "plt.plot(x, t_post.pdf(x))\n",
    "plt.xticks(np.arange(-0.5, 0.01, 0.5))\n",
    "plt.fill_between(x, t_post.pdf(x), 0, facecolor='blue', alpha=.2)\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlabel(r\"Mean difference ($\\mu$)\")\n",
    "plt.title(\"Posterior distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "better_prob = 1 - t_post.cdf(0)\n",
    "\n",
    "print(f\"Probability of {model_scores.index[0]} being more accurate than \"\n",
    "      f\"{model_scores.index[1]}: {better_prob:.3f}\")\n",
    "print(f\"Probability of {model_scores.index[1]} being more accurate than \"\n",
    "      f\"{model_scores.index[0]}: {1 - better_prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1228ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rope_interval = [-0.01, 0.01]\n",
    "rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n",
    "\n",
    "print(f\"Probability of {model_scores.index[0]} and {model_scores.index[1]} \"\n",
    "      f\"being practically equivalent: {rope_prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765853f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rope = np.linspace(rope_interval[0], rope_interval[1], 100)\n",
    "\n",
    "plt.plot(x, t_post.pdf(x))\n",
    "plt.xticks(np.arange(-0.5, 0.01, 0.5))\n",
    "plt.vlines([-0.01, 0.01], ymin=0, ymax=(np.max(t_post.pdf(x)) + 1))\n",
    "plt.fill_between(x_rope, t_post.pdf(x_rope), 0, facecolor='blue', alpha=.2)\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlabel(r\"Mean difference ($\\mu$)\")\n",
    "plt.title(\"Posterior distribution under the ROPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_intervals = []\n",
    "intervals = [0.5, 0.75, 0.95]\n",
    "\n",
    "for interval in intervals:\n",
    "    cred_interval = list(t_post.interval(interval))\n",
    "    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\n",
    "\n",
    "cred_int_df = pd.DataFrame(\n",
    "    cred_intervals,\n",
    "    columns=['interval', 'lower value', 'upper value']\n",
    ").set_index('interval')\n",
    "cred_int_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from math import factorial\n",
    "\n",
    "n_comparisons = (\n",
    "    factorial(len(model_scores))\n",
    "    / (factorial(2) * factorial(len(model_scores) - 2))\n",
    ")\n",
    "\n",
    "pairwise_bayesian = []\n",
    "\n",
    "for model_i, model_k in combinations(range(len(model_scores)), 2):\n",
    "    model_i_scores = model_scores.iloc[model_i].values\n",
    "    model_k_scores = model_scores.iloc[model_k].values\n",
    "    differences = model_i_scores - model_k_scores\n",
    "    t_post = t(\n",
    "        df, loc=np.mean(differences),\n",
    "        scale=corrected_std(differences, n_train, n_test)\n",
    "    )\n",
    "    worse_prob = t_post.cdf(rope_interval[0])\n",
    "    better_prob = 1 - t_post.cdf(rope_interval[1])\n",
    "    rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n",
    "\n",
    "    pairwise_bayesian.append([model_scores.index[model_i], model_scores.index[model_k], worse_prob, better_prob, rope_prob])\n",
    "\n",
    "pairwise_bayesian_df = (pd.DataFrame(\n",
    "    pairwise_bayesian,\n",
    "    columns=['model_1', 'model_2','worse_prob', 'better_prob', 'rope_prob']\n",
    ").round(3))\n",
    "\n",
    "pairwise_bayesian_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
