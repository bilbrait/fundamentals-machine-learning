{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.datasets as data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' ) # use whitegrid if prefer a white background\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#matplotlib.rcParams.update( { 'font.size': 18 } ) # Use this to setup your preferred font size for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b199f7",
   "metadata": {},
   "source": [
    "#### 1- Use a Linear SVM to fit a model on the Iris dataset. Try different train/test partitions and $C$ values and plot the found supporting vectors along with the 0-1 loss error. Also compute how many support vectors are used on a given train/test split to generate the separating hyperplane. \n",
    "\n",
    "+ It will be easier to use only two features at a time from the data and in a binary classification context. Also try out different feature and class combinations to see how the model behaves with different feature options. For example: sepal (width vs length) | petal (width vs length ) | sepal width vs petal width | ...\n",
    "+ Use the supporting vectors to extract the used data from the training set and examine the features (for example, comparing mean values between classes, mean absolute deviance, ... ). Compare also the initial training set with the  extracted data.\n",
    "+ Recommend using Pandas and Seaborn libraries to to some explorative data plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910bacd3",
   "metadata": {},
   "source": [
    "### Iris data\n",
    "\n",
    "Number of Instances\n",
    "150 (50 in each of three classes)\n",
    "\n",
    "Number of Attributes\n",
    "4 numeric, predictive attributes and the class\n",
    "\n",
    "Attribute Information\n",
    "sepal length in cm\n",
    "\n",
    "sepal width in cm\n",
    "\n",
    "petal length in cm\n",
    "\n",
    "petal width in cm\n",
    "\n",
    "class:\n",
    "Iris-Setosa\n",
    "\n",
    "Iris-Versicolour\n",
    "\n",
    "Iris-Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.load_iris( return_X_y = True )\n",
    "\n",
    "\n",
    "X = X[y != 0, :2] # Sepal width and length columns.\n",
    "y = y[y != 0]     # Versicolour vs Virginicia\n",
    "\n",
    "Xdf              = pd.DataFrame( X, columns = [ 'sepal width', 'sepal length' ] )\n",
    "Xdf[ 'Species' ] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aeda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot( data = Xdf[ [ 'sepal width', 'sepal length' ] ] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80578a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopted from Scikit-Learn examples: \n",
    "# https://scikit-learn.org/stable/auto_examples/index.html\n",
    "\n",
    "from sklearn.svm import SVC # See help( SVC ) for class details\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = ?, random_state = rng.integers( 421 ) )\n",
    "\n",
    "C   = ?\n",
    "clf = SVC( kernel = 'linear', C = C)\n",
    "clf.fit( ?,? )\n",
    "\n",
    "\n",
    "plt.figure( figsize = ( 10, 7 ) )\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# Plot support vectors\n",
    "plt.scatter(?, ?, s=80,\n",
    "                facecolors='none', zorder=10, edgecolors='r')\n",
    "    \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n",
    "                edgecolor='k', s=20)\n",
    "\n",
    "# Circle out the test data\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',\n",
    "                zorder=10, edgecolor='g')\n",
    "\n",
    "plt.axis('tight')\n",
    "x_min = X[:, 0].min()\n",
    "x_max = X[:, 0].max()\n",
    "y_min = X[:, 1].min()\n",
    "y_max = X[:, 1].max()\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "                linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n",
    "\n",
    "\n",
    "plt.title( 'Linear SVM, C = {0}, 0-1 loss {1}'.format( C, zero_one_loss( y_test, clf.predict( X_test ) ) ) )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c75ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the datapoints from the training data and labels using the support vector points\n",
    "Xsv = ?\n",
    "ysv = ?\n",
    "Xsv = pd.DataFrame( Xsv, columns = [ 'sepal width', 'sepal length' ] )\n",
    "\n",
    "Xsv[ 'Species' ] = ysv\n",
    "Xsv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same basic data statistic plotting as in the beginning with seaborn but using the support vector observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844a6be",
   "metadata": {},
   "source": [
    "#### 2- Interpolate a sine function with a 3rd degree polynomial using Backpropagation.  That is, Interpolate the function $f(x) = sin(x)$ where $x\\in[-\\pi,\\pi]$\n",
    "\n",
    "+ For each polynomial coefficient $W_k$, setup a random weight\n",
    "+ Forward pass compute the desired polynomial coefficient estimats $\\hat{y} = W_0 + W_1x_1 + W_2 x_2 + W_3 x_3$, for example\n",
    "+ Backward pass, compute the change 2 * ($\\hat{y} - y$), and update the coefficients $W_k$\n",
    "+ Update weights by adjustig with $W_k = W_k - learning rate * W_k$\n",
    "+ Experiment with different learning rates and number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "X = np.linspace( -math.pi, math.pi, k )\n",
    "y = np.sin( X )\n",
    "\n",
    "W0 = ?\n",
    "?\n",
    "?\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4422949",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-6 # learning rate, that is in what increments (or jumps) the search is done along the data\n",
    "\n",
    "T = 2000 # Number of BP iterations\n",
    "for i in range( T ):\n",
    "    \n",
    "    # Forward pass\n",
    "    yhat = ?\n",
    "    \n",
    "    loss = np.square( ? - ? ).sum()\n",
    "    #if t % 100 == 99: print( t, loss )\n",
    "        \n",
    "    # Backward pass\n",
    "    d_yhat = ?\n",
    "    d_W0   = d_yhat.sum()\n",
    "    d_W1   = ( d_yhat * X ).sum()\n",
    "    d_W2   = ( d_yhat * X ** 2 ).sum()\n",
    "    d_W3   = ( d_yhat * X ** 3).sum()\n",
    "    \n",
    "    # Update network weights\n",
    "    W0 -= ?\n",
    "print()\n",
    "print(f'Result: yhat = {W0} + {W1} x + {W2} x^2 + {W3} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74697eaa",
   "metadata": {},
   "source": [
    "#### 2- Use A Neural Network to model digital images. Use different layer sizes, activation functions, and regularizations and examine how these affect the resulting weights and train & test scores. What reveals more structure in the weights? Feel free to vary anyother settings in the model. \n",
    "+ Bonus: Experiment how adding Gaussian and Laplacian noise affects the regularization and structures of the gradients in the layers.\n",
    "  + np.random.laplace, np.random.normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier # Use help( MLPClassifier )\n",
    "\n",
    "\n",
    "Xmnist, Ymnist = data.fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e50a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_lap = np.random.laplace( 0, 1, Xmnist.shape )\n",
    "Xmnist = Xmnist + noise_lap\n",
    "Xminst = Xmnist / 255.\n",
    "mlp = MLPClassifier( hidden_layer_sizes = (?,), activation = ?, max_iter = ? )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( Xmnist, Ymnist, test_size=?, random_state= rng.integers( 521 ) )\n",
    "\n",
    "# This is for catching and ignoring if fitting produces an convergence issue.\n",
    "# Comment away if you want verbose warnings.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                            module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "print( \"Training score: {0}\".format( mlp.score( X_train, y_train ) ) )\n",
    "print( \"Test score: {0}\".format( mlp.score(X_test, y_test ) ) )\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize = ( 10, 7 ) )\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f836e8",
   "metadata": {},
   "source": [
    "#### 3- Experiment the effects of regularization on Neural Networks using different learning rates, number of iterations, number of hidden layers and neurons, and activiation functions. Use the datasets provided and compare convergence rates with the statistics observed in the datasets.\n",
    "\n",
    "Example and code adopted from Scikit-learn's Neural Network example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438fb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = data.load_iris()\n",
    "X_digits, y_digits = data.load_digits(return_X_y=True)\n",
    "data_sets = [ (iris.data, iris.target), (X_digits, y_digits),\n",
    "              data.make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "              data.make_moons(noise=0.3, random_state=0),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import warnings\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# different learning rate schedules and momentum parameters. Add more solvers and vary their parameters.\n",
    "# remember to match labels and plot_args to the number of different solvers.\n",
    "params = [\n",
    "    {\n",
    "        \"solver\": \"sgd\",\n",
    "        \"learning_rate\": \"constant\",\n",
    "        \"momentum\": 0,\n",
    "        \"learning_rate_init\": 0.2,\n",
    "    },\n",
    "    \n",
    "    {\"solver\": \"adam\", \"learning_rate_init\": 0.01},\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"constant learning-rate\",\n",
    "    \"adam\",\n",
    "]\n",
    "\n",
    "plot_args = [\n",
    "    {\"c\": \"red\", \"linestyle\": \"-\"},\n",
    "    {\"c\": \"black\", \"linestyle\": \"-\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb79df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_on_dataset(X, y, ax, name):\n",
    "    # for each dataset, plot learning for each learning strategy\n",
    "    print(\"\\nlearning on dataset %s\" % name)\n",
    "    ax.set_title(name)\n",
    "\n",
    "    X = MinMaxScaler().fit_transform(X) # Try also without variable scaling\n",
    "    mlps = []\n",
    "    \n",
    "    for label, param in zip(labels, params):\n",
    "        print(\"training: %s\" % label)\n",
    "        mlp = MLPClassifier( random_state=0, hidden_layer_sizes= ( ?), activation = ?,\n",
    "                             max_iter= ? , **param) # Change \n",
    "\n",
    "        # some parameter combinations will not converge as can be seen on the\n",
    "        # plots so they are ignored here\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\", category=ConvergenceWarning, module=\"sklearn\"\n",
    "            )\n",
    "            mlp.fit(X, y)\n",
    "\n",
    "        mlps.append(mlp)\n",
    "        print(\"Training set score: %f\" % mlp.score(X, y))\n",
    "        print(\"Training set loss: %f\" % mlp.loss_)\n",
    "    for mlp, label, args in zip(mlps, labels, plot_args):\n",
    "        ax.plot(mlp.loss_curve_, label=label, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c593a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "\n",
    "for ax, data, name in zip(\n",
    "    axes.ravel(), data_sets, [\"iris\", \"digits\", \"circles\", \"moons\"]\n",
    "):\n",
    "    plot_on_dataset(*data, ax=ax, name=name)\n",
    "\n",
    "fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
