{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ab20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' )\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "\n",
    "rng = np.random.RandomState( 12445124 )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f71695",
   "metadata": {},
   "source": [
    "#### First some warm up tasks. When in doubt, the help() function is your friend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a82da",
   "metadata": {},
   "source": [
    "#### 1- Create Gaussian and Cauchy random variable vectors of length $n$. For each vector, plot $\\frac{\\sum_i^k x_i}{k}, k=1..n$. When does the sequence stabilizes and what can you observe?\n",
    "\n",
    "#### ( Do not go $n \\geq 1000000$ if you want to save time. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import cauchy\n",
    "\n",
    "n = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244ee7a",
   "metadata": {},
   "source": [
    "#### 2- Create two vectors of Gaussian random variables and perform a linear regression, using scipy.stats.linregress. Perform the regression, for example, 1000 times and plot the correlation coffecients and p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf0f3a",
   "metadata": {},
   "source": [
    "#### 3- Using the Iris data set, use a supervised Ridge regression to estimate class membership probabilities. Choose only two classes.\n",
    "\n",
    "#### Plot what would be the Rademacher complexity for the given linear model? What would be the estimated probability of error using the sample size given by the Rademacher complexity?\n",
    "\n",
    "P( X, Y | error ) = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a45217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as data\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "iris = data.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bccc3",
   "metadata": {},
   "source": [
    "#### 4-  Run each model, on the Iris data set, $rounds=k$ times using different train/test partitions and take the avegarge of their error rates. Plot and compare the models below using 0-1 metric on the predicted data and labelled data. Experiment with different number of $k$ rounds.\n",
    "\n",
    "+ Use default definitions of the models.\n",
    "+ For plotting: use train set proportion on the x-axis and probability of error on the y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "heldout = [ 0.90, 0.50, 0.02 ] # suggested starting point. Modify to your liking.\n",
    "rounds  = 30 # Try out different amount of rounds\n",
    "\n",
    "classifiers = [\n",
    "    (\"SGD\", SGDClassifier(max_iter=110)),\n",
    "    (\"Perceptron\", Perceptron(max_iter=110)),\n",
    "    (\n",
    "        \"Passive-Aggressive I\",\n",
    "        PassiveAggressiveClassifier(max_iter=110, loss=\"hinge\", C=1.0, tol=1e-4),\n",
    "    ),\n",
    "    (\n",
    "        \"Passive-Aggressive II\",\n",
    "        PassiveAggressiveClassifier( max_iter=110, loss=\"squared_hinge\", C=1.0, tol=1e-4 ),\n",
    "    ),\n",
    "    (\n",
    "        \"SAG\",\n",
    "        LogisticRegression(max_iter=110, solver=\"sag\", tol=1e-1, C=1.0e4 / X.shape[0]),\n",
    "    ),\n",
    "]\n",
    "\n",
    "xx = 1.0 - np.array(heldout)\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    print(\"training %s\" % name)\n",
    "    yy = []\n",
    "    for i in heldout:\n",
    "        yy_ = []\n",
    "        for r in range(rounds):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=i, random_state = rng.integers( 142 )\n",
    "            )\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            yy_.append( metrics.zero_one_loss( y_test, y_pred, normalize = False ) )\n",
    "        yy.append(np.mean(yy_))\n",
    "    plt.plot(xx, yy, label=name)\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Proportion train\")\n",
    "plt.ylabel(\"0-1 loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bac85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "090d035a",
   "metadata": {},
   "source": [
    "#### 5a- Repeat the experiments on task 4, but on a single train/test split, for each model separatelyh and plot the model's error rates in a histogram.\n",
    "\n",
    "#### 5b- Plot the training and test curves of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(name, clf) = classifiers[ 0 ]\n",
    "\n",
    "yy = []\n",
    "    \n",
    "yy_ = []\n",
    "for r in range( rounds ):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rng )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    yy_.append( metrics.zero_one_loss( y_test, y_pred ) )\n",
    "\n",
    "plt.hist(yy_, bins = np.unique( yy_ ) )\n",
    "plt.title(\"SGD - Bar of 0-1 loss with 80/20 split\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62562f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes_abs, train_scores, test_scores = learning_curve(\n",
    "    clf,\n",
    "    X,\n",
    "    y,\n",
    "    train_sizes=np.linspace(0.1, 1, 10),\n",
    "    scoring= None,\n",
    "    cv=5,\n",
    "    n_jobs = 4\n",
    ")\n",
    "\n",
    "train_sizes=np.linspace(0.1, 1, 10)\n",
    "\n",
    "plt.plot(train_sizes, train_scores.mean(1), \"o-\", color=\"r\", label=\"{0} Train\".format(name))\n",
    "plt.plot(train_sizes, test_scores.mean(1), \"o-\", color=\"g\", label=\"{0} Test\".format(name))\n",
    "plt.xlabel(\"Train size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning curves\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db1fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
