{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13562de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329cdb0d",
   "metadata": {},
   "source": [
    "#### 1- See how round-off errors behave when computing a derivative of a function. Use $f_0 = sin( x_0 )$ and compute its derivative with different discretization ranges. Experiment with different points $x_0$ of differentiation.\n",
    "\n",
    "Steps:\n",
    "+ Choose a point of differentation $x_0$\n",
    "+ Choose a function $f_0$. Here $sin$ is given.\n",
    "+ Assign the derivative $f_0$ to, for example, $f_p$\n",
    "+ Choose a range $i = [ -20, 0 ]$, with 0.5 steps, of discretizations $h = 10^{(\\text{list of i})}$\n",
    "+ Compute the absolute error: $|f_p - \\frac{f_0( x_0+h ) - f_0 }{h}|$\n",
    "+ Compare the absolute error against the discretization error without round-off errors $\\frac{1}{2}|f^{''}(x_0)|h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.2\n",
    "f0 = math.sin( x0 )\n",
    "fp = math.cos( x0 )\n",
    "i  = np.arange( -20, 0, 0.5 )\n",
    "h  = np.power( 10, i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d61ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = [ abs( fp - ( math.sin( x0+h0 ) - f0 ) / h0 ) for h0 in h ] # Tip: Use list comprehension\n",
    "derr = f0 / 2 * h # Discretization error without roundoff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4333ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog( h, err, '-ob' )\n",
    "plt.loglog( h, derr, '--r' )\n",
    "plt.title( f'Discretation and round-off error: x0: {x0}, f0: Sin(x0), fp: Cos(x0)')\n",
    "plt.xlabel( 'Discretization h' )\n",
    "plt.ylabel( 'Absolute error' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4973e1",
   "metadata": {},
   "source": [
    "#### 2- Linear least-squares revisited on polynomial fitting with normal equations. Compare the polynomial coefficients to the backpropagation method from the previous exercise.\n",
    "\n",
    "Solve the algebraic problem of $\\underset{\\vec{x}}{\\text{min}} ||\\vec{b}-A\\vec{x}||_2, \\ A \\in \\mathbb{R}^{m \\times n}, \\ \\vec{x} \\in \\mathbb{R}^n, \\ \\vec{b} \\in \\mathbb{R}^m, m \\geq n$\n",
    "\n",
    "+ Create $B = A^T A$ (a), and $\\vec{y} = A^T \\vec{b}$ (b)\n",
    "+ Use Cholesky Factorization for solving $B$. That is, for $B = GG^T$:\n",
    "    + Solve lower triangular system $G\\vec{z} = \\vec{y}$ for $\\vec{z}$ (c)\n",
    "    + Solve upper triangular system $G^T\\vec{x} = \\vec{z}$ for $\\vec{x}$ (d)\n",
    "    + Reminder/tip: Lower triangular system: below the diagnoal of a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae93f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(t, b, n):\n",
    "    t = t.reshape(-1, 1)\n",
    "    b = b.reshape(-1, 1)\n",
    "    m = len(t)\n",
    "    A = np.ones((m, n))\n",
    "    for j in range(1, n):\n",
    "        A[:, j] = A[:, j-1] * t.flatten()\n",
    "    B = A.T @ A # (a)\n",
    "    y = A.T @ b # (b)\n",
    "    coefs = np.linalg.solve(B, y)\n",
    "    return coefs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 20\n",
    "d = 4 # max degree of polynomial fitting\n",
    "tt = np.linspace(0, 1, m)\n",
    "bb = np.sin(2 * np.pi * tt)\n",
    "\n",
    "coefs = {}\n",
    "for n in range(1, d+1):\n",
    "    coefs[n] = least_squares_fit(tt, bb, n)\n",
    "\n",
    "t = np.linspace(0, 1, 101)\n",
    "z = np.ones((d, 101))\n",
    "# (c) lower triangular matrix G\n",
    "for n in range(1, d+1):\n",
    "    z[n-1, :] = z[n-1, :] * coefs[n][n-1]\n",
    "    # (d) upper triangular matrix G^T\n",
    "    for j in range(n-2, -1, -1):\n",
    "        z[n-1, :] = z[n-1, :] * t + coefs[n][j]\n",
    "\n",
    "plt.plot(t, z.T, tt, bb, 'ro')\n",
    "plt.plot( tt, bb, label = 'data')\n",
    "plt.legend()\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('p_{n-1}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f566f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = np.random.randn()\n",
    "W1 = np.random.randn()\n",
    "W2 = np.random.randn()\n",
    "W3 = np.random.randn()\n",
    "W4 = np.random.randn()\n",
    "\n",
    "lr = 1e-6 # learning rate\n",
    "\n",
    "T = 2000 # Number of BP iterations\n",
    "for i in range( T ):\n",
    "    \n",
    "    # Forward pass\n",
    "    yhat = W0 + W1*tt + W2*tt**2 + W3*tt**3 + W4*tt**4\n",
    "    \n",
    "    loss = np.square( yhat - bb ).sum()\n",
    "    #if t % 100 == 99: print( t, loss )\n",
    "        \n",
    "    # Backward pass\n",
    "    d_yhat = 2.0 * ( yhat - bb )\n",
    "    d_W0   = d_yhat.sum()\n",
    "    d_W1   = ( d_yhat * tt ).sum()\n",
    "    d_W2   = ( d_yhat * tt ** 2 ).sum()\n",
    "    d_W3   = ( d_yhat * tt ** 3).sum()\n",
    "    d_W4   = ( d_yhat * tt ** 4).sum()\n",
    "    \n",
    "    # Update network weights\n",
    "    W0 -= lr * d_W0\n",
    "    W1 -= lr * d_W1\n",
    "    W2 -= lr * d_W2\n",
    "    W3 -= lr * d_W3\n",
    "    W4 -= lr * d_W4\n",
    "    \n",
    "print()\n",
    "print(f'Result: yhat = {W0} + {W1} x + {W2} x^2 + {W3} x^3 + {W4} x^4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146485b0",
   "metadata": {},
   "source": [
    "#### 3- Eigenvalue solvers for blind source separation: How sensitive Principal Component Analysis (PCA) and Independent Component Analysis (ICA) are to the number of observations, given a random process? What changes the most in both PCA and ICA projections?\n",
    "\n",
    "##### Solving the eigenvalue problem $A\\vec{x} = \\lambda \\vec{x}, \\ A \\in \\mathbb{R}^2, \\ \\vec{x}, \\lambda \\in \\mathbb{R}$\n",
    "\n",
    "##### Both PCA and ICA:uses Singular Value Decomposition method for extracting relevant eigenvalues and vectors from a given data: $A = U \\Sigma V^T$, where $U,V$ are the left and right orthonormal bases vectors of the eigenvalues $\\Sigma$, which is sorted from high to low. \n",
    "\n",
    "Try out:\n",
    "+ Student t Distribution ( https://en.wikipedia.org/wiki/Student's_t-distribution )\n",
    "+ Pareto Distribution ( https://en.wikipedia.org/wiki/Pareto_distribution )\n",
    "+ As a bonus 1: check other distributions from np_rng class. Use external sources to checkout properties of other distributions if you're going to use other than the two above.\n",
    "+ Bonus 2: Check out the ratio of the largest and smallest eigenvalues of the observed data, using np.linalg.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81715fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "np_rng = np.random.RandomState( 4432 )\n",
    "\n",
    "S      = np_rng.pareto(1.5, size=(20000, 2)) # Student t Distribution\n",
    "S[:, 0] *= 2.\n",
    "\n",
    "# Mix data\n",
    "A = np.array([[1, 1], [0, 2]])  # Mixing matrix. Vary the numbers\n",
    "\n",
    "# Generate observations by computing the matrix product SA^T\n",
    "X = np.dot(S, A.T)  \n",
    "\n",
    "pca    = PCA()\n",
    "S_pca_ = pca.fit(X).transform(X)\n",
    "\n",
    "ica    = FastICA(random_state=rng.integers( 42 ) )\n",
    "S_ica_ = ica.fit(X).transform(X)  # Estimate the sources\n",
    "\n",
    "# Scale row-wise using standard deviation of S_ica_\n",
    "S_ica_ /= S_ica_.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(S, axis_list=None):\n",
    "    plt.scatter(S[:, 0], S[:, 1], s=2, marker='o', zorder=10,\n",
    "                color='steelblue', alpha=0.5)\n",
    "    if axis_list is not None:\n",
    "        colors = ['orange', 'red']\n",
    "        for color, axis in zip(colors, axis_list):\n",
    "            axis /= axis.std()\n",
    "            x_axis, y_axis = axis\n",
    "            # Trick to get legend to work\n",
    "            plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color)\n",
    "            plt.quiver((0, 0), (0, 0), x_axis, y_axis, zorder=11, width=0.01,\n",
    "                       scale=6, color=color)\n",
    "\n",
    "    plt.hlines(0, -3, 3)\n",
    "    plt.vlines(0, -3, 3)\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 14, 9 ) )\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_samples(S / S.std())\n",
    "plt.title('True Independent Sources')\n",
    "\n",
    "# PCA components and ICA mixing components\n",
    "axis_list = [pca.components_.T, ica.mixing_]\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_samples(X / np.std(X), axis_list=axis_list)\n",
    "legend = plt.legend(['PCA', 'ICA'], loc='upper right')\n",
    "legend.set_zorder(100)\n",
    "\n",
    "plt.title('Observations')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_samples(S_pca_ / np.std(S_pca_, axis=0))\n",
    "plt.title('PCA recovered signals')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_samples(S_ica_ / np.std(S_ica_))\n",
    "plt.title('ICA recovered signals')\n",
    "\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903d541-0cb2-423d-bec1-cd0c739c5125",
   "metadata": {},
   "source": [
    "#### 4- Implement a logistic regression using automatic differentiation. Use for example the Iris dataset from sklearn.datasets. You can use other datasets too, as long as it is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845190f-650c-4cc4-a797-712dd825a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79bd52",
   "metadata": {},
   "source": [
    "#### 4- Implement a linear regression using automatic differentiation. Try with different learning rates and number of iterations for the gradient descent steps. How sensitive the model is to the learning rate and iterations?\n",
    "+ (a) implement Wx + b\n",
    "+ (b) implement Mean squared error ( hint: np.square )\n",
    "+ (c) vary number of points n and noise level. How sensitive the method is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33cf291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear regression model: Xw + b ( hint: dot product )\n",
    "def model(X, w, b):\n",
    "    return np.dot( X, w ) + b # (a)\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "def loss(params, X, y):\n",
    "    return np.mean( np.square(model(X, params[0], params[1]) - y) )\n",
    "\n",
    "# Create a gradient function for the loss function\n",
    "loss_grad = grad( loss ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c090aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data using a list of integers\n",
    "n     = 10 #(c)\n",
    "X     = np.arange(n).reshape(-1, 1)  # Reshape to 10x1 vector\n",
    "noise = np.random.randn(n, 1) # Add some noise (c)\n",
    "y      = 2 * X + 1 + noise \n",
    "\n",
    "print('Num of data points:', n)\n",
    "print('X shape:', X.shape)\n",
    "print('noise shape:', noise.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases randomly\n",
    "w = np.random.rand(1, 1)\n",
    "b = np.random.rand(1, 1)\n",
    "\n",
    "# Perform gradient descent\n",
    "learning_rate = 0.01\n",
    "for i in range(100):\n",
    "    grad_w, grad_b = loss_grad([w, b], X, y)\n",
    "    print(f'i: {i}, w={w}, loss={loss([w, b], X, y)}, grad_w={grad_w}, grad_b={grad_b}')\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "# Print the learned weights and biases\n",
    "print(\"Learned w:\", w)\n",
    "print(\"Learned b:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the same data\n",
    "predictions = model(X, w, b)\n",
    "\n",
    "# Compare predicted and real values\n",
    "print(\"Predicted values:\", predictions)\n",
    "print(\"Actual values:\", y)\n",
    "\n",
    "# Visualize the comparison (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X, y, 'o', label='Actual data')\n",
    "plt.plot(X, predictions, '-x', label='Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
