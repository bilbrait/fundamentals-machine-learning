{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13562de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329cdb0d",
   "metadata": {},
   "source": [
    "#### 1- See how round-off errors behave when computing a derivative of a function. Use $f_0 = sin( x_0 )$ and compute its derivative with different discretization ranges. Experiment with different points $x_0$ of differentiation.\n",
    "\n",
    "Steps:\n",
    "+ Choose a point of differentation $x_0$\n",
    "+ Choose a function $f_0$. Here $sin$ is given.\n",
    "+ Assign the derivative $f_0$ to, for example, $f_p$\n",
    "+ Choose a range $i = [ -20, 0 ]$, with 0.5 steps, of discretizations $h = 10^{(\\text{list of i})}$\n",
    "+ Compute the absolute error: $|f_p - \\frac{f_0( x_0+h ) - f_0 }{h}|$\n",
    "+ Compare the absolute error against the discretization error without round-off errors $\\frac{1}{2}|f^{''}(x_0)|h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.2\n",
    "f0 = math.sin( x0 )\n",
    "fp = math.cos( x0 )\n",
    "i  = np.arange( -20, 0, 0.5 )\n",
    "h  = np.power( 10, i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d61ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = [ abs( fp - ( math.sin( x0+h0 ) - f0 ) / h0 ) for h0 in h ] # Tip: Use list comprehension\n",
    "derr = f0 / 2 * h # Discretization error without roundoff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4333ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog( h, err, '-ob' )\n",
    "plt.loglog( h, derr, '--r' )\n",
    "plt.title( f'Discretation and round-off error: x0: {x0}, f0: Sin(x0), fp: Cos(x0)')\n",
    "plt.xlabel( 'Discretization h' )\n",
    "plt.ylabel( 'Absolute error' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4973e1",
   "metadata": {},
   "source": [
    "#### 2- Linear least-squares revisited on polynomial fitting with normal equations. Compare the polynomial coefficients to the backpropagation method from the previous exercise.\n",
    "\n",
    "Solve the algebraic problem of $\\underset{\\vec{x}}{\\text{min}} ||\\vec{b}-A\\vec{x}||_2, \\ A \\in \\mathbb{R}^{m \\times n}, \\ \\vec{x} \\in \\mathbb{R}^n, \\ \\vec{b} \\in \\mathbb{R}^m, m \\geq n$\n",
    "\n",
    "+ Create $B = A^T A$ (a), and $\\vec{y} = A^T \\vec{b}$ (b)\n",
    "+ Use Cholesky Factorization for solving $B$. That is, for $B = GG^T$:\n",
    "    + Solve lower triangular system $G\\vec{z} = \\vec{y}$ for $\\vec{z}$ (c)\n",
    "    + Solve upper triangular system $G^T\\vec{x} = \\vec{z}$ for $\\vec{x}$ (d)\n",
    "    + Reminder/tip: Lower triangular system: below the diagnoal of a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae93f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_fit(t, b, n):\n",
    "    t = t.reshape(-1, 1)\n",
    "    b = b.reshape(-1, 1)\n",
    "    m = len(t)\n",
    "    A = np.ones((m, n))\n",
    "    for j in range(1, n):\n",
    "        A[:, j] = A[:, j-1] * t.flatten()\n",
    "    B = A.T @ A # (a)\n",
    "    y = A.T @ b # (b)\n",
    "    coefs = np.linalg.solve(B, y)\n",
    "    return coefs.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 20\n",
    "d = 4 # max degree of polynomial fitting\n",
    "tt = np.linspace(0, 1, m)\n",
    "bb = np.sin(2 * np.pi * tt)\n",
    "\n",
    "coefs = {}\n",
    "for n in range(1, d+1):\n",
    "    coefs[n] = least_squares_fit(tt, bb, n)\n",
    "\n",
    "t = np.linspace(0, 1, 101)\n",
    "z = np.ones((d, 101))\n",
    "# (c) lower triangular matrix G\n",
    "for n in range(1, d+1):\n",
    "    z[n-1, :] = z[n-1, :] * coefs[n][n-1]\n",
    "    # (d) upper triangular matrix G^T\n",
    "    for j in range(n-2, -1, -1):\n",
    "        z[n-1, :] = z[n-1, :] * t + coefs[n][j]\n",
    "\n",
    "plt.plot(t, z.T, tt, bb, 'ro')\n",
    "plt.plot( tt, bb, label = 'data')\n",
    "plt.legend()\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('p_{n-1}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f566f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = np.random.randn()\n",
    "W1 = np.random.randn()\n",
    "W2 = np.random.randn()\n",
    "W3 = np.random.randn()\n",
    "W4 = np.random.randn()\n",
    "\n",
    "lr = 1e-6 # learning rate\n",
    "\n",
    "T = 2000 # Number of BP iterations\n",
    "for i in range( T ):\n",
    "    \n",
    "    # Forward pass\n",
    "    yhat = W0 + W1*tt + W2*tt**2 + W3*tt**3 + W4*tt**4\n",
    "    \n",
    "    loss = np.square( yhat - bb ).sum()\n",
    "    #if t % 100 == 99: print( t, loss )\n",
    "        \n",
    "    # Backward pass\n",
    "    d_yhat = 2.0 * ( yhat - bb )\n",
    "    d_W0   = d_yhat.sum()\n",
    "    d_W1   = ( d_yhat * tt ).sum()\n",
    "    d_W2   = ( d_yhat * tt ** 2 ).sum()\n",
    "    d_W3   = ( d_yhat * tt ** 3).sum()\n",
    "    d_W4   = ( d_yhat * tt ** 4).sum()\n",
    "    \n",
    "    # Update network weights\n",
    "    W0 -= lr * d_W0\n",
    "    W1 -= lr * d_W1\n",
    "    W2 -= lr * d_W2\n",
    "    W3 -= lr * d_W3\n",
    "    W4 -= lr * d_W4\n",
    "    \n",
    "print()\n",
    "print(f'Result: yhat = {W0} + {W1} x + {W2} x^2 + {W3} x^3 + {W4} x^4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146485b0",
   "metadata": {},
   "source": [
    "#### 3- Eigenvalue solvers for blind source separation: How sensitive Principal Component Analysis (PCA) and Independent Component Analysis (ICA) are to the number of observations, given a random process? What changes the most in both PCA and ICA projections?\n",
    "\n",
    "##### Solving the eigenvalue problem $A\\vec{x} = \\lambda \\vec{x}, \\ A \\in \\mathbb{R}^2, \\ \\vec{x}, \\lambda \\in \\mathbb{R}$\n",
    "\n",
    "##### Both PCA and ICA:uses Singular Value Decomposition method for extracting relevant eigenvalues and vectors from a given data: $A = U \\Sigma V^T$, where $U,V$ are the left and right orthonormal bases vectors of the eigenvalues $\\Sigma$, which is sorted from high to low. \n",
    "\n",
    "Try out:\n",
    "+ Student t Distribution ( https://en.wikipedia.org/wiki/Student's_t-distribution )\n",
    "+ Pareto Distribution ( https://en.wikipedia.org/wiki/Pareto_distribution )\n",
    "+ As a bonus 1: check other distributions from np_rng class. Use external sources to checkout properties of other distributions if you're going to use other than the two above.\n",
    "+ Bonus 2: Check out the ratio of the largest and smallest eigenvalues of the observed data, using np.linalg.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81715fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "np_rng = np.random.RandomState( 4432 )\n",
    "\n",
    "S      = np_rng.pareto(1.5, size=(20000, 2)) # Student t Distribution\n",
    "S[:, 0] *= 2.\n",
    "\n",
    "# Mix data\n",
    "A = np.array([[1, 1], [0, 2]])  # Mixing matrix. Vary the numbers\n",
    "\n",
    "# Generate observations by computing the matrix product SA^T\n",
    "X = np.dot(S, A.T)  \n",
    "\n",
    "pca    = PCA()\n",
    "S_pca_ = pca.fit(X).transform(X)\n",
    "\n",
    "ica    = FastICA(random_state=rng.integers( 42 ) )\n",
    "S_ica_ = ica.fit(X).transform(X)  # Estimate the sources\n",
    "\n",
    "# Scale row-wise using standard deviation of S_ica_\n",
    "S_ica_ /= S_ica_.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(S, axis_list=None):\n",
    "    plt.scatter(S[:, 0], S[:, 1], s=2, marker='o', zorder=10,\n",
    "                color='steelblue', alpha=0.5)\n",
    "    if axis_list is not None:\n",
    "        colors = ['orange', 'red']\n",
    "        for color, axis in zip(colors, axis_list):\n",
    "            axis /= axis.std()\n",
    "            x_axis, y_axis = axis\n",
    "            # Trick to get legend to work\n",
    "            plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color)\n",
    "            plt.quiver((0, 0), (0, 0), x_axis, y_axis, zorder=11, width=0.01,\n",
    "                       scale=6, color=color)\n",
    "\n",
    "    plt.hlines(0, -3, 3)\n",
    "    plt.vlines(0, -3, 3)\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 14, 9 ) )\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_samples(S / S.std())\n",
    "plt.title('True Independent Sources')\n",
    "\n",
    "# PCA components and ICA mixing components\n",
    "axis_list = [pca.components_.T, ica.mixing_]\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_samples(X / np.std(X), axis_list=axis_list)\n",
    "legend = plt.legend(['PCA', 'ICA'], loc='upper right')\n",
    "legend.set_zorder(100)\n",
    "\n",
    "plt.title('Observations')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_samples(S_pca_ / np.std(S_pca_, axis=0))\n",
    "plt.title('PCA recovered signals')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_samples(S_ica_ / np.std(S_ica_))\n",
    "plt.title('ICA recovered signals')\n",
    "\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903d541-0cb2-423d-bec1-cd0c739c5125",
   "metadata": {},
   "source": [
    "#### 4- Implement a logistic regression using automatic differentiation. Use for example the Iris dataset from sklearn.datasets. You can use other datasets too, as long as it is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5845190f-650c-4cc4-a797-712dd825a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autograd\n",
      "  Downloading autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from autograd) (1.26.4)\n",
      "Downloading autograd-1.7.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: autograd\n",
      "Successfully installed autograd-1.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c3679c-2be8-4e15-8ba8-887203bbf849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.31471805599453\n",
      "Trained loss: -34.657359027997266\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from autograd.test_util import check_grads\n",
    "import sklearn.datasets as data\n",
    "\n",
    "X, y = data.load_iris( return_X_y = True )\n",
    "\n",
    "\n",
    "X = X[y != 0, :3] # Sepal width and length columns.\n",
    "y = y[y != 0]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (np.tanh(x) + 1)\n",
    "\n",
    "\n",
    "def logistic_predictions(weights, X):\n",
    "    # Outputs probability of a label being true according to logistic model.\n",
    "    return sigmoid(np.dot(X, weights))\n",
    "\n",
    "\n",
    "def training_loss(weights):\n",
    "    # Training loss is the negative log-likelihood of the training labels.\n",
    "    preds = logistic_predictions(weights, X)\n",
    "    label_probabilities = preds * y + (1 - preds) * (1 - y)\n",
    "    return -np.sum(np.log(label_probabilities))\n",
    "\n",
    "\n",
    "# Build a toy dataset.\n",
    "#inputs = np.array([[0.52, 1.12, 0.77], [0.88, -1.08, 0.15], [0.52, 0.06, -1.30], [0.74, -2.49, 1.39]])\n",
    "#targets = np.array([True, True, False, True])\n",
    "\n",
    "# Build a function that returns gradients of training loss using autograd.\n",
    "training_gradient_fun = grad(training_loss)\n",
    "\n",
    "# Check the gradients numerically, just to be safe.\n",
    "weights = np.array([0.0, 0.0, 0.0])\n",
    "check_grads(training_loss, modes=[\"rev\"])(weights)\n",
    "\n",
    "# Optimize weights using gradient descent.\n",
    "print(\"Initial loss:\", training_loss(weights))\n",
    "for i in range(100):\n",
    "    weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "print(\"Trained loss:\", training_loss(weights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
