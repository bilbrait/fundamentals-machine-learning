{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.datasets as data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb; sb.set_style( 'darkgrid' ) # use whitegrid if prefer a white background\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "rng = default_rng( SeedSequence().entropy )\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#matplotlib.rcParams.update( { 'font.size': 18 } ) # Use this to setup your preferred font size for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b199f7",
   "metadata": {},
   "source": [
    "#### 1- Use a Linear SVM to fit a model on the Iris dataset. Try different train/test partitions and $C$ values and plot the found supporting vectors along with the 0-1 loss error. Also compute how many support vectors are used on a given train/test split to generate the separating hyperplane. \n",
    "\n",
    "+ It will be easier to use only two features at a time from the data and in a binary classification context. Also try out different feature and class combinations to see how the model behaves with different feature options. For example: sepal (width vs length) | petal (width vs length ) | sepal width vs petal width | ...\n",
    "+ Use the supporting vectors to extract the used data from the training set and examine the features (for example, comparing mean values between classes, mean absolute deviance, ... ). Compare also the initial training set with the  extracted data.\n",
    "+ Recommend using Pandas and Seaborn libraries to to some explorative data plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910bacd3",
   "metadata": {},
   "source": [
    "### Iris data\n",
    "\n",
    "Number of Instances\n",
    "150 (50 in each of three classes)\n",
    "\n",
    "Number of Attributes\n",
    "4 numeric, predictive attributes and the class\n",
    "\n",
    "Attribute Information\n",
    "sepal length in cm\n",
    "\n",
    "sepal width in cm\n",
    "\n",
    "petal length in cm\n",
    "\n",
    "petal width in cm\n",
    "\n",
    "class:\n",
    "Iris-Setosa\n",
    "\n",
    "Iris-Versicolour\n",
    "\n",
    "Iris-Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = data.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y != 0, :2] # Sepal width and length columns.\n",
    "y = y[y != 0]     # Versicolour vs Virginicia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot( data = pd.DataFrame( X ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80578a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # See help( SVC ) for class details\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "np.random.seed( 241542 )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state = rng.integers( 421 ) )\n",
    "\n",
    "C   = 1.0\n",
    "clf = SVC( kernel = 'linear', C = C)\n",
    "clf.fit( X_train, y_train )\n",
    "\n",
    "\n",
    "plt.figure( figsize = ( 10, 7 ) )\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# Plot support vectors\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
    "                facecolors='none', zorder=10, edgecolors='r')\n",
    "    \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n",
    "                edgecolor='k', s=20)\n",
    "\n",
    "# Circle out the test data\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',\n",
    "                zorder=10, edgecolor='g')\n",
    "\n",
    "plt.axis('tight')\n",
    "x_min = X[:, 0].min()\n",
    "x_max = X[:, 0].max()\n",
    "y_min = X[:, 1].min()\n",
    "y_max = X[:, 1].max()\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "                linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n",
    "\n",
    "\n",
    "plt.title( 'Linear SVM, C = {0}, 0-1 loss {1}'.format( C, zero_one_loss( y_test, clf.predict( X_test ) ) ) )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c75ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsv = X_train[ clf.support_ ]\n",
    "Xdf = pd.DataFrame( Xsv, columns = [ 'sepal width', 'sepal length' ] )\n",
    "Xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61356bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot( data = Xdf );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74697eaa",
   "metadata": {},
   "source": [
    "#### 2- Use A Neural Network to model digital images. Use different layer sizes, activation functions, and regularizations and examine how these affect the resulting weights and train & test scores. What reveals more structure in the weights? Feel free to vary anyother settings in the model. \n",
    "+ Bonus: Experiment how adding Gaussian and Laplacian noise affects the regularization and structures of the gradients in the layers.\n",
    "  + np.random.laplace, np.random.normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier # Use help( MLPClassifier )\n",
    "\n",
    "\n",
    "Xmnist, Ymnist = data.fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e50a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_lap = np.random.laplace( 0, 1, Xmnist.shape )\n",
    "Xmnist = Xmnist + noise_lap\n",
    "Xminst = Xmnist / 255.\n",
    "mlp = MLPClassifier( hidden_layer_sizes = 30, activation = 'relu', max_iter = 100 )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( Xmnist, Ymnist, test_size=0.1, random_state= rng.integers( 521 ) )\n",
    "\n",
    "# This is for catching and ignoring if fitting produces an convergence issue.\n",
    "# Comment away if you want verbose warnings.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
    "                            module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "print( \"Training score: {0}\".format( mlp.score( X_train, y_train ) ) )\n",
    "print( \"Test score: {0}\".format( mlp.score(X_test, y_test ) ) )\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize = ( 10, 7 ) )\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62666678",
   "metadata": {},
   "source": [
    "#### 3- Experiment the effects of regularization on SVMs using both the $C$Â and $gamma$ parameters. Use the Iris dataset and observe how the the regularization parameter affects the model fitting. Plot the the best coefficient into the train and error curves, and report the number of support vectors\n",
    "+ Add Gaussian noise to the observations to see how regularization affects the results\n",
    "+ Fix either C or gamma and vary the other parameter\n",
    "+ Use also different train/test splits\n",
    "+ Bonus: extract the support vector indicises, like in Task 1, and inpsect the statistics of found support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c28611",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data.load_iris( return_X_y = True )\n",
    "\n",
    "X += np.random.normal( 0, 11, X.shape )\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split( X, Y, test_size = 0.4, random_state = rng.integers( 42 ) )\n",
    "\n",
    "param_range = np.logspace( -10, 1, 10 ) # gamma parameter\n",
    "C           = 122.0\n",
    "svc         = SVC( C = C, kernel = 'linear' )\n",
    "train_e     = []\n",
    "test_e      = []\n",
    "\n",
    "for a in param_range:\n",
    "    svc.set_params( gamma = a )\n",
    "    svc.fit( Xtrain, ytrain )\n",
    "    train_e.append( svc.score( Xtrain, ytrain ) )\n",
    "    test_e.append( svc.score( Xtest, ytest ) )\n",
    "    \n",
    "i_optim = np.argmax(test_e)\n",
    "optim   = param_range[i_optim]\n",
    "print(\"Optimal regularization parameter : %s\" % optim)\n",
    "\n",
    "svc.set_params( gamma = optim)\n",
    "svc.fit( X, Y )\n",
    "print( \"Number of support vectors per class:\")\n",
    "print( \"Class 0: {0} | Class 1: {1}\".format( svc.n_support_[ 0 ], svc.n_support_[ 1 ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0968c",
   "metadata": {},
   "source": [
    "##### Plot here ther train and error curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 17, 10 ) )\n",
    "plt.semilogx(param_range, train_e, label='Train')\n",
    "plt.semilogx(param_range, test_e, label='Test')\n",
    "plt.vlines(optim, plt.ylim()[0], np.max(test_e), color='k',\n",
    "           linewidth=5, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([min(test_e)-0.1, max(train_e)+0.1])\n",
    "plt.xlabel('Regularization parameter')\n",
    "plt.ylabel('Performance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe699f",
   "metadata": {},
   "source": [
    "##### Plot here the decision surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8bd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 10, 7 ) )\n",
    "plt.clf()\n",
    "# Plot support vectors\n",
    "plt.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=80,\n",
    "                facecolors='none', zorder=10, edgecolors='r')\n",
    "    \n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,\n",
    "                edgecolor='k', s=20)\n",
    "\n",
    "plt.axis('tight')\n",
    "x_min = X[:, 0].min()\n",
    "x_max = X[:, 0].max()\n",
    "y_min = X[:, 1].min()\n",
    "y_max = X[:, 1].max()\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "                linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n",
    "\n",
    "\n",
    "plt.title( 'Linear SVM, C = {0}, gamma = {1}'.format( C, optim ) )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
